{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Token e4342ac4fcf98c2e1910b122cb4103c059f8bbfc',\n",
    "}\n",
    "\n",
    "response = requests.get('https://bilishorturl.ml/api/projects/3/export?exportType=JSON', headers=headers)\n",
    "\n",
    "import json\n",
    "annotations = json.loads(response.content)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "keypoints_mapping = {}\n",
    "\n",
    "x_grid_size = 20\n",
    "y_grid_size = 35\n",
    "grid_num = x_grid_size * y_grid_size\n",
    "\n",
    "unknown_idx = 0\n",
    "\n",
    "\n",
    "def getCenter(keypoints):\n",
    "    for point in keypoints:\n",
    "        point['center_x'] = point['x'] + point['width'] / 2 \n",
    "        point['center_y'] = point['y'] + point['height'] / 2\n",
    "\n",
    "# return_interpolation: When true append whether interpolated at the end\n",
    "# 1 means exist, 0 means missing\n",
    "def interpolation(keypoints, frames):\n",
    "    prev = keypoints[0]['frame'] - 1\n",
    "    prev_x = 0\n",
    "    prev_y = 0\n",
    "    res = np.zeros((frames,1))\n",
    "    for i in keypoints:\n",
    "        diff = i['frame'] - prev\n",
    "        cur_x = i['center_x']\n",
    "        cur_y = i['center_y']\n",
    "        cur = i['frame']\n",
    "        for j in range(prev + 1, i['frame']):\n",
    "            # tmp = {'frame': j}\n",
    "            tmp_x = (prev_x * (cur - j) + cur_x * (j - prev)) / diff\n",
    "            tmp_y = (prev_y * (cur - j) + cur_y * (j - prev)) / diff\n",
    "\n",
    "            res[j - 1, 0] = min(int(tmp_x / 100 * x_grid_size) * y_grid_size + int(tmp_y / 100 * y_grid_size) + 1, grid_num)\n",
    "            # if return_interpolation:\n",
    "            #     res[j - 1, -1] = 1\n",
    "            # tmp['interpolated'] = True\n",
    "            # res.append(tmp)\n",
    "        res[cur - 1, 0] = min(int(cur_x / 100 * x_grid_size) * y_grid_size + int(cur_y / 100 * y_grid_size) + 1, grid_num)\n",
    "        # if return_interpolation:\n",
    "        #     res[cur - 1, -1] = 1\n",
    "        prev_x = cur_x\n",
    "        prev_y = cur_y\n",
    "        prev = i['frame']\n",
    "\n",
    "    return res\n",
    "\n",
    "def process_seq(boxes):\n",
    "    wand_end_keypoint = None\n",
    "    wand_tip_keypoint = None\n",
    "    wand_end_framesCount = None\n",
    "    wand_tip_framesCount = None\n",
    "\n",
    "    for i in boxes:\n",
    "        if 'labels' not in i['value'].keys():\n",
    "            continue\n",
    "        if i['value']['labels'][0] == labels_name[0]:\n",
    "            wand_tip_keypoint = i['value']['sequence']\n",
    "            wand_tip_framesCount = i['value']['framesCount']\n",
    "        elif i['value']['labels'][0] == labels_name[1]:\n",
    "            wand_end_keypoint = i['value']['sequence']\n",
    "            wand_end_framesCount = i['value']['framesCount']\n",
    "    \n",
    "    assert wand_tip_keypoint and wand_end_keypoint, f\"missing annotations for {annotation['id']}\"\n",
    "    assert wand_end_framesCount == wand_tip_framesCount, f'frames not matched for {annotation[\"id\"]}'\n",
    "\n",
    "    framesCount = wand_end_framesCount\n",
    "    # assert boxes[0]['value']['framesCount'] == boxes[1]['value']['framesCount'], f'frames not matched for {annotation[\"id\"]}'\n",
    "    # assert len(boxes) >= 2, f\"missing annotations for {annotation['id']}\"\n",
    "\n",
    "    \n",
    "    getCenter(wand_end_keypoint)\n",
    "\n",
    "    wand_end_keypoint = interpolation(wand_end_keypoint, framesCount)\n",
    "\n",
    "\n",
    "    getCenter(wand_tip_keypoint)\n",
    "\n",
    "    wand_tip_keypoint = interpolation(wand_tip_keypoint, framesCount)\n",
    "\n",
    "\n",
    "    return framesCount, wand_end_keypoint, wand_tip_keypoint\n",
    "\n",
    "\n",
    "labels_name = ['wand tip', 'wand end']\n",
    "\n",
    "for annotation in annotations:\n",
    "    vid_name = annotation['file_upload']\n",
    "\n",
    "    # boxes = annotation['annotations'][0]['result']\n",
    "    \n",
    "    framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(annotation['annotations'][0]['result'])\n",
    "\n",
    "    concat_keypoint = np.zeros(shape=(framesCount, 2))\n",
    "    concat_keypoint[:, :1] = wand_end_keypoint\n",
    "    concat_keypoint[:, 1:] = wand_tip_keypoint\n",
    "\n",
    "    \n",
    "    keypoints_mapping[vid_name] = torch.tensor(concat_keypoint)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/projects/3/tasks/?page_size=-1', headers=headers)\n",
    "\n",
    "all_tasks = json.loads(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/predictions', headers=headers)\n",
    "\n",
    "assert response.status_code == 200, \"connection error\"\n",
    "\n",
    "all_predictions = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'model_version', 'created_ago', 'result', 'score', 'cluster', 'neighbors', 'mislabeling', 'created_at', 'updated_at', 'task'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory that contains original videos\n",
    "\n",
    "import os\n",
    "source_dir = \"G:/.shortcut-targets-by-id/1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT/Data Wizards/dataset/processed_vid\"\n",
    "category_mapping = {\"3-24 V\": 0, \"3-25 bridge\": 1, \"3-25 R\": 2, \"Accio\": 1, \"Avada Kedavra\": 3, \"Invalid\": 4, \"Lumos\": 0, \"Revelio\": 2}\n",
    "\n",
    "vid_class = {} # name in processed_vid : category\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    tmp_root = root[root.rfind('/') + 1: ]\n",
    "    tmp_root = tmp_root[tmp_root.rfind('\\\\') + 1: ]\n",
    "    category = None if tmp_root not in category_mapping.keys() else category_mapping[tmp_root]\n",
    "    for name in files:\n",
    "        if not name.endswith('mp4'):\n",
    "            continue\n",
    "        assert category is not None, f\"No label at{os.path.join(root, name)} {tmp_root}\"\n",
    "\n",
    "        vid_class[name] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMG_6458.mp4', 'IMG_6459_8.mp4', 'IMG_6458_2.mp4', 'IMG_6458_4.mp4', 'IMG_6458_6.mp4', 'IMG_6458_7.mp4', 'IMG_6458_5.mp4', 'IMG_6458_9.mp4', 'IMG_6458_8.mp4', 'IMG_6458_3.mp4', 'IMG_6458_11.mp4', 'IMG_6458_10.mp4', 'IMG_6458_12.mp4', 'IMG_6458_14.mp4', 'IMG_6458_13.mp4', 'IMG_6458_15.mp4', 'IMG_6458_16.mp4', 'IMG_6458_18.mp4', 'IMG_6458_19.mp4', 'IMG_6458_17.mp4', 'IMG_6459.mp4', 'IMG_6459_2.mp4', 'IMG_6459_3.mp4', 'IMG_6459_4.mp4', 'IMG_6459_5.mp4', 'IMG_6459_6.mp4', 'IMG_6459_7.mp4', 'IMG_6459_9.mp4', 'IMG_6459_10.mp4', 'IMG_6459_11.mp4', 'IMG_6460_9.mp4', 'IMG_6460_10.mp4', 'IMG_6460_11.mp4', 'IMG_6460_12.mp4', 'IMG_6460_13.mp4', 'IMG_6460_14.mp4', 'IMG_6460_15.mp4', 'IMG_6460_16.mp4', 'IMG_6460_17.mp4', 'IMG_6460_18.mp4', 'IMG_6460_19.mp4', 'IMG_6460_20.mp4', 'IMG_6460_21.mp4', 'IMG_6460_22.mp4', 'IMG_6460_23.mp4', 'IMG_6460_24.mp4', 'IMG_6460_25.mp4', 'IMG_6460_26.mp4', 'IMG_6460.mp4', 'IMG_6460_2.mp4', 'IMG_6460_3.mp4', 'IMG_6460_4.mp4', 'IMG_6460_5.mp4', 'IMG_6460_6.mp4', 'IMG_6460_7.mp4', 'IMG_6460_8.mp4', 'IMG_6408_37_.mp4', 'IMG_6408_39_.mp4', 'IMG_6408_34_.mp4', 'IMG_6408_35_.mp4', 'IMG_6408_41_.mp4', 'IMG_6408_33_.mp4', 'IMG_6408_36_.mp4', 'IMG_6408_38_.mp4', 'IMG_6408_40_.mp4', 'IMG_6408_32_.mp4', 'IMG_6408_31_.mp4', 'IMG_6408_30_.mp4', 'IMG_6408_28_.mp4', 'IMG_6408_29_.mp4', 'IMG_6408_24_.mp4', 'IMG_6408_27_.mp4', 'IMG_6408_26_.mp4', 'IMG_6408_25_.mp4', 'IMG_6408_20_.mp4', 'IMG_6408_22_.mp4', 'IMG_6408_21_.mp4', 'IMG_6408_23_.mp4', 'IMG_6408_19_.mp4', 'IMG_6408_14_.mp4', 'IMG_6408_15_.mp4', 'IMG_6408_18_.mp4', 'IMG_6408_17_.mp4', 'IMG_6408_16_.mp4', 'IMG_6408_13_.mp4', 'IMG_6408_11_.mp4', 'IMG_6408_12_.mp4', 'IMG_6408_10_.mp4', 'IMG_6408_9_.mp4', 'IMG_6408_7_.mp4', 'IMG_6408_8_.mp4', 'IMG_6408_5_.mp4', 'IMG_6408_6_.mp4', 'IMG_6408_3_.mp4', 'IMG_6408_1_.mp4', 'IMG_6408_4_.mp4', 'IMG_6408_2_.mp4', 'IMG_6408.mp4', 'IMG_1616.mp4', 'IMG_1607.mp4', 'IMG_1605.mp4', 'IMG_1612.mp4', 'IMG_1611.mp4', 'IMG_1613.mp4', 'IMG_1608.mp4', 'IMG_1614.mp4', 'IMG_1604.mp4', 'IMG_1609.mp4', 'IMG_1615.mp4', 'IMG_1610.mp4', '2023-03-26_04_54_10.mp4', '2023-03-26_04_54_03.mp4', '2023-03-26_04_53_56.mp4', '2023-03-26_04_53_42.mp4', '2023-03-26_04_53_49.mp4', '2023-03-26_04_59_23.mp4', '2023-03-26_04_59_09.mp4', '2023-03-26_04_59_16.mp4', '2023-03-26_04_59_02.mp4', '2023-03-26_04_58_48.mp4', '2023-03-26_04_58_55.mp4', '2023-03-26_04_58_41.mp4', '2023-03-26_04_58_26.mp4', '2023-03-26_04_58_34.mp4', '2023-03-26_04_58_19.mp4', '2023-03-26_04_57_03.mp4', '2023-03-26_04_56_49.mp4', '2023-03-26_04_56_41.mp4', '2023-03-26_04_56_56.mp4', '2023-03-26_04_56_34.mp4', '2023-03-26_04_56_27.mp4', '2023-03-26_04_56_20.mp4', '2023-03-26_04_56_13.mp4', '2023-03-26_04_56_06.mp4', '2023-03-26_04_54_46.mp4', '2023-03-26_04_54_39.mp4', '2023-03-26_04_54_32.mp4', '2023-03-26_04_54_18.mp4', '2023-03-26_04_54_25.mp4', '2023-03-26_04_55_59.mp4', '04-06-2023-1.mp4', '04-06-2023-2.mp4', '04-06-2023-28.mp4', '04-06-2023-26.mp4', '04-06-2023-24.mp4', '04-06-2023-22.mp4', '04-06-2023-21.mp4', '04-06-2023-20.mp4', '04-06-2023-19.mp4', '04-06-2023-18.mp4', '04-06-2023-17.mp4', '04-06-2023-16.mp4', '04-06-2023-14.mp4', '04-06-2023-13.mp4', '04-06-2023-10.mp4', '04-06-2023-9.mp4', '04-06-2023-8.mp4', '04-06-2023-7.mp4', '04-06-2023-6.mp4', '04-06-2023-5.mp4', '04-06-2023-29.mp4', '04-06-2023-27.mp4', '04-06-2023-4.mp4', '04-06-2023-23.mp4', '04-06-2023-25.mp4', '04-06-2023-15.mp4', '04-06-2023-11.mp4', '04-06-2023-12.mp4', '04-06-2023-3.mp4', '04-06-2023-36.mp4', '04-06-2023-35.mp4', '04-06-2023-34.mp4', '04-06-2023-33.mp4', '04-06-2023-32.mp4', '04-06-2023-31.mp4', '04-06-2023-30.mp4', 'IMG_1631.mp4', 'IMG_1632.mp4', 'IMG_1635.mp4', 'IMG_1634.mp4', 'IMG_1626.mp4', 'IMG_1627.mp4', 'IMG_1633.mp4', 'IMG_1628.mp4', 'IMG_1630.mp4', 'IMG_1629.mp4', '2023-03-26_05_12_55.mp4', '2023-03-26_05_13_02.mp4', '2023-03-26_05_12_48.mp4', '2023-03-26_05_12_41.mp4', '2023-03-26_05_12_19.mp4', '2023-03-26_05_12_34.mp4', '2023-03-26_05_12_27.mp4', '2023-03-26_05_12_12.mp4', '2023-03-26_05_12_05.mp4', '2023-03-26_05_11_58.mp4', '2023-03-26_05_11_36.mp4', '2023-03-26_05_11_51.mp4', '2023-03-26_05_11_44.mp4', '2023-03-26_05_11_29.mp4', '2023-03-26_05_11_22.mp4', '2023-03-26_05_11_15.mp4', '2023-03-26_05_11_08.mp4', '2023-03-26_05_11_01.mp4', '2023-03-26_05_10_54.mp4', '2023-03-26_05_10_46.mp4', '2023-03-26_05_08_43.mp4', '2023-03-26_05_08_36.mp4', '2023-03-26_05_08_29.mp4', '2023-03-26_05_08_21.mp4', '2023-03-26_05_08_00.mp4', '2023-03-26_05_07_53.mp4', '2023-03-26_05_07_46.mp4', '2023-03-26_05_07_38.mp4', 'IMG_1639.mp4', 'IMG_1640.mp4', 'IMG_1643.mp4', 'IMG_1644.mp4', 'IMG_1646.mp4', 'IMG_1638.mp4', 'IMG_1641.mp4', 'IMG_1645.mp4', 'IMG_1647.mp4', 'IMG_1642.mp4', '2023-03-26_05_24_29.mp4', '2023-03-26_05_24_58.mp4', '2023-03-26_05_24_36.mp4', '2023-03-26_05_24_22.mp4', '2023-03-26_05_24_50.mp4', '2023-03-26_05_24_43.mp4', '2023-03-26_05_24_15.mp4', '2023-03-26_05_24_08.mp4', '2023-03-26_05_24_01.mp4', '2023-03-26_05_23_53.mp4', '2023-03-26_05_23_32.mp4', '2023-03-26_05_23_39.mp4', '2023-03-26_05_23_46.mp4', '2023-03-26_05_23_25.mp4', '2023-03-26_05_23_11.mp4', '2023-03-26_05_23_18.mp4', '2023-03-26_05_23_04.mp4', '2023-03-26_05_22_56.mp4', '2023-03-26_05_22_49.mp4', '2023-03-26_05_22_42.mp4', '2023-03-26_05_26_09.mp4', '2023-03-26_05_26_02.mp4', '2023-03-26_05_25_55.mp4', '2023-03-26_05_25_47.mp4', '2023-03-26_05_25_40.mp4', '2023-03-26_05_25_33.mp4', '2023-03-26_05_25_26.mp4', '2023-03-26_05_25_19.mp4', '2023-03-26_05_25_12.mp4', '2023-03-26_05_25_05.mp4', 'IMG_1661.mp4', 'IMG_1659.mp4', 'IMG_1653.mp4', 'IMG_1649.mp4', 'IMG_1657.mp4', 'IMG_1655.mp4', 'IMG_1660.mp4', 'IMG_1654.mp4', 'IMG_1658.mp4', 'IMG_1651.mp4', 'IMG_1648.mp4', 'IMG_1652.mp4', 'IMG_1656.mp4', 'IMG_1650.mp4', '2023-03-26_05_21_09.mp4', '2023-03-26_05_21_02.mp4', '2023-03-26_05_20_54.mp4', '2023-03-26_05_20_47.mp4', '2023-03-26_05_20_40.mp4', '2023-03-26_05_20_33.mp4', '2023-03-26_05_20_26.mp4', '2023-03-26_05_20_19.mp4', '2023-03-26_05_20_12.mp4', '2023-03-26_05_20_05.mp4', '2023-03-26_05_18_04.mp4', '2023-03-26_05_17_57.mp4', '2023-03-26_05_17_50.mp4', '2023-03-26_05_17_43.mp4', '2023-03-26_05_17_36.mp4', '2023-03-26_05_17_29.mp4', '2023-03-26_05_17_22.mp4', '2023-03-26_05_17_14.mp4', '2023-03-26_05_17_00.mp4', '2023-03-26_05_17_07.mp4', '2023-03-26_05_16_53.mp4', '2023-03-26_05_16_32.mp4', '2023-03-26_05_16_46.mp4', '2023-03-26_05_16_25.mp4', '2023-03-26_05_16_39.mp4', '2023-03-26_05_16_17.mp4', '2023-03-26_05_16_10.mp4', '2023-03-26_05_16_03.mp4', '2023-03-26_05_15_56.mp4', '2023-03-26_05_15_49.mp4', '04-06-2023-40.mp4', '04-06-2023-39.mp4', '04-06-2023-38.mp4', '04-06-2023-37.mp4', 'IMG_1618.mp4', '2023-03-26_05_31_19.mp4', '2023-03-26_05_31_12.mp4', '2023-03-26_05_31_05.mp4', '2023-03-26_05_30_58.mp4', '2023-03-26_05_30_51.mp4', '2023-03-26_05_30_44.mp4', '2023-03-26_05_30_30.mp4', '2023-03-26_05_30_37.mp4', '2023-03-26_05_30_15.mp4', '2023-03-26_05_30_22.mp4', '2023-03-26_05_30_08.mp4', '2023-03-26_05_29_54.mp4', '2023-03-26_05_30_01.mp4', '2023-03-26_05_29_47.mp4', '2023-03-26_05_29_40.mp4', '2023-03-26_05_28_27.mp4', '2023-03-26_05_28_20.mp4', '2023-03-26_05_28_13.mp4', '2023-03-26_05_27_58.mp4', '2023-03-26_05_28_06.mp4', '2023-03-26_05_27_51.mp4', '2023-03-26_05_27_37.mp4', '2023-03-26_05_27_44.mp4', '2023-03-26_05_27_30.mp4', '2023-03-26_05_27_23.mp4'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_class.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_seq(j, pad_length = 90, device = device):\n",
    "    res = torch.zeros(size=(pad_length, j.shape[1]), dtype=torch.long)\n",
    "    padded = pad_length - j.shape[0]\n",
    "    res[int(padded / 2): int(padded / 2) + j.shape[0],:] = j\n",
    "    return res.to(device)\n",
    "\n",
    "\n",
    "# keypoints_mapping_padded = {}\n",
    "dataset = []\n",
    "\n",
    "for file_name, j in keypoints_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    dataset.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long, device = device)))\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "d = int(len(dataset) * 0.85)\n",
    "\n",
    "\n",
    "train = dataset[:d]\n",
    "val = dataset[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./runs/detect/train5/weights/best.pt\"\n",
    "\n",
    "keypoints_pred_mapping = {}\n",
    "\n",
    "for i in all_tasks:\n",
    "    file_name = i['data']['video']\n",
    "    file_name = file_name[file_name.rfind('/') + 1:]\n",
    "    preds = i['predictions']\n",
    "    for j in preds[::-1]:\n",
    "        if j['model_version'] == model_name:\n",
    "            framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(j['result'])\n",
    "            concat_keypoint = np.zeros(shape=(framesCount, 2))\n",
    "            concat_keypoint[:, :1] = wand_end_keypoint\n",
    "            concat_keypoint[:, 1:] = wand_tip_keypoint\n",
    "\n",
    "            keypoints_pred_mapping[file_name] = torch.tensor(concat_keypoint, dtype=torch.long)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = []\n",
    "missing = 0\n",
    "\n",
    "\n",
    "for file_name, j in keypoints_pred_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    if file_name in vid_class.keys():\n",
    "        dataset_pred.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long, device=device)))\n",
    "    # else:\n",
    "    #     missing += 1\n",
    "    #     print(file_name)\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset_pred)\n",
    "d = int(len(dataset_pred) * 0.85)\n",
    "\n",
    "\n",
    "train_pred = dataset_pred[:d]\n",
    "val_pred = dataset_pred[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]\n",
    "    \n",
    "train_dataset = ClsDataset(train_pred)\n",
    "val_dataset = ClsDataset(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self, embed_size = 30, num_layers = 2):\n",
    "        super(lstm, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(x_grid_size * y_grid_size + 1, embed_size)\n",
    "        self.rnn1 = nn.LSTM(input_size = embed_size, hidden_size = 100, batch_first = True, bidirectional = True, dropout = 0.1, num_layers = num_layers)\n",
    "        self.rnn2 = nn.LSTM(input_size = embed_size, hidden_size = 100, batch_first = True, bidirectional = True, dropout = 0.1, num_layers = num_layers)\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(400, 200)\n",
    "        self.fc2 = nn.Linear(200, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.rnn(x)[1][0].shape)\n",
    "        # return self.fc(torch.squeeze(self.rnn(x)[1][0], dim=0))\n",
    "        # print(self.rnn(x)[0].shape)\n",
    "        return self.fc2(self.relu( \\\n",
    "            self.fc1(self.dropout(torch.cat((self.rnn1(self.embedding(x[:, :, 0]))[0][:, -1, :], self.rnn2(self.embedding(x[:, :, 1]))[0][:, -1, :]), dim=-1)))))\n",
    "\n",
    "\n",
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"rnn\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat((torch.zeros(size=(1,100)), torch.zeros(size=(1,100))), dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = lstm().to(device)\n",
    "optimizer = Adam(get_grouped_params(model, 0.1), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 19, 30, 11, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = [0] * 5\n",
    "for feature, label in train:\n",
    "    cnts[label.item()] += 1\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 6, 2, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = [0] * 5\n",
    "for feature, label in val:\n",
    "    cnts[label.item()] += 1\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.5172413793103449 train loss:  0.26009746037778403 train acc: 0.9507692307692308\n",
      "val acc:  0.5 train loss:  0.2616824238073258 train acc: 0.9507692307692308\n",
      "val acc:  0.4827586206896552 train loss:  0.25179934359732126 train acc: 0.9538461538461539\n",
      "val acc:  0.5172413793103449 train loss:  0.25324368689741406 train acc: 0.9569230769230769\n",
      "val acc:  0.5 train loss:  0.2561209652395475 train acc: 0.96\n",
      "val acc:  0.5689655172413793 train loss:  0.27246719421375365 train acc: 0.9415384615384615\n",
      "val acc:  0.5 train loss:  0.27491606984819683 train acc: 0.9415384615384615\n",
      "val acc:  0.5689655172413793 train loss:  0.26415610987515675 train acc: 0.9476923076923077\n",
      "val acc:  0.5862068965517241 train loss:  0.24805322786172232 train acc: 0.96\n",
      "val acc:  0.5172413793103449 train loss:  0.2519179696128482 train acc: 0.9476923076923077\n",
      "val acc:  0.5172413793103449 train loss:  0.24409754893609456 train acc: 0.9661538461538461\n",
      "val acc:  0.5344827586206896 train loss:  0.25641526707581114 train acc: 0.9476923076923077\n",
      "val acc:  0.5344827586206896 train loss:  0.24567900669007076 train acc: 0.963076923076923\n",
      "val acc:  0.5517241379310345 train loss:  0.2380129276286988 train acc: 0.9569230769230769\n",
      "val acc:  0.5344827586206896 train loss:  0.25174678436347414 train acc: 0.9446153846153846\n",
      "val acc:  0.5 train loss:  0.2338488804442542 train acc: 0.96\n",
      "val acc:  0.5517241379310345 train loss:  0.24985956436111814 train acc: 0.9507692307692308\n",
      "val acc:  0.5517241379310345 train loss:  0.2732330681312652 train acc: 0.9323076923076923\n",
      "val acc:  0.5172413793103449 train loss:  0.24462262505576723 train acc: 0.9538461538461539\n",
      "val acc:  0.5344827586206896 train loss:  0.2346789035059157 train acc: 0.963076923076923\n",
      "val acc:  0.5517241379310345 train loss:  0.3023464942262286 train acc: 0.9292307692307692\n",
      "val acc:  0.5862068965517241 train loss:  0.26798466841379803 train acc: 0.9476923076923077\n",
      "val acc:  0.5689655172413793 train loss:  0.2313222988020806 train acc: 0.9692307692307692\n",
      "val acc:  0.5862068965517241 train loss:  0.24202548925365722 train acc: 0.9661538461538461\n",
      "val acc:  0.5344827586206896 train loss:  0.33261965215206146 train acc: 0.9353846153846154\n",
      "val acc:  0.5 train loss:  0.35435432621410917 train acc: 0.92\n",
      "val acc:  0.5172413793103449 train loss:  0.2901387271426973 train acc: 0.9446153846153846\n",
      "val acc:  0.5172413793103449 train loss:  0.2863395136027109 train acc: 0.96\n",
      "val acc:  0.5172413793103449 train loss:  0.28354710766247343 train acc: 0.9538461538461539\n",
      "val acc:  0.5344827586206896 train loss:  0.259619686575163 train acc: 0.9661538461538461\n",
      "val acc:  0.5517241379310345 train loss:  0.2805181982971373 train acc: 0.9507692307692308\n",
      "val acc:  0.5344827586206896 train loss:  0.2509377999674706 train acc: 0.9538461538461539\n",
      "val acc:  0.5172413793103449 train loss:  0.2546500748112088 train acc: 0.9569230769230769\n",
      "val acc:  0.5344827586206896 train loss:  0.21655520832254774 train acc: 0.9723076923076923\n",
      "val acc:  0.5517241379310345 train loss:  0.20797757591520036 train acc: 0.9753846153846154\n",
      "val acc:  0.5862068965517241 train loss:  0.20124866494110652 train acc: 0.9723076923076923\n",
      "val acc:  0.5344827586206896 train loss:  0.1950896910968281 train acc: 0.9784615384615385\n",
      "val acc:  0.5689655172413793 train loss:  0.18288148123593556 train acc: 0.9846153846153847\n",
      "val acc:  0.5517241379310345 train loss:  0.19118597216549374 train acc: 0.9784615384615385\n",
      "val acc:  0.5517241379310345 train loss:  0.1749500385707333 train acc: 0.9815384615384616\n",
      "val acc:  0.5517241379310345 train loss:  0.1833182583962168 train acc: 0.9846153846153847\n",
      "val acc:  0.5344827586206896 train loss:  0.17167713280235017 train acc: 0.9815384615384616\n",
      "val acc:  0.5517241379310345 train loss:  0.16902000386090504 train acc: 0.9876923076923076\n",
      "val acc:  0.5172413793103449 train loss:  0.17398884963421596 train acc: 0.9846153846153847\n",
      "val acc:  0.5689655172413793 train loss:  0.18153017794802076 train acc: 0.9846153846153847\n",
      "val acc:  0.5862068965517241 train loss:  0.16855672959770476 train acc: 0.9846153846153847\n",
      "val acc:  0.5689655172413793 train loss:  0.1669637273464884 train acc: 0.9815384615384616\n",
      "val acc:  0.5689655172413793 train loss:  0.15944763876142956 train acc: 0.9846153846153847\n",
      "val acc:  0.5862068965517241 train loss:  0.15438657537812278 train acc: 0.9907692307692307\n",
      "val acc:  0.5862068965517241 train loss:  0.1576229426122847 train acc: 0.9907692307692307\n",
      "val acc:  0.5172413793103449 train loss:  0.3135416961851574 train acc: 0.9076923076923077\n",
      "val acc:  0.4827586206896552 train loss:  0.25125860174496967 train acc: 0.963076923076923\n",
      "val acc:  0.5689655172413793 train loss:  0.2306173230920519 train acc: 0.96\n",
      "val acc:  0.5517241379310345 train loss:  0.20021947366850718 train acc: 0.9723076923076923\n",
      "val acc:  0.5689655172413793 train loss:  0.17989730622087205 train acc: 0.9846153846153847\n",
      "val acc:  0.5517241379310345 train loss:  0.16094379730167843 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.16196661016770772 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.15761242558558783 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.16762161006530127 train acc: 0.9784615384615385\n",
      "val acc:  0.5517241379310345 train loss:  0.15505031922033854 train acc: 0.9846153846153847\n",
      "val acc:  0.5344827586206896 train loss:  0.15026914009026118 train acc: 0.9876923076923076\n",
      "val acc:  0.5517241379310345 train loss:  0.14759915180149533 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.15275851335553897 train acc: 0.9907692307692307\n",
      "val acc:  0.5862068965517241 train loss:  0.15096691286280042 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.14577817774954296 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.14284210297323408 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.13950586975330398 train acc: 0.9907692307692307\n",
      "val acc:  0.5689655172413793 train loss:  0.15300106150763376 train acc: 0.9815384615384616\n",
      "val acc:  0.5689655172413793 train loss:  0.15730550920679456 train acc: 0.9846153846153847\n",
      "val acc:  0.5862068965517241 train loss:  0.22027660941793806 train acc: 0.9507692307692308\n",
      "val acc:  0.5862068965517241 train loss:  0.24699495377994718 train acc: 0.9446153846153846\n",
      "val acc:  0.5862068965517241 train loss:  0.18438481894277392 train acc: 0.9753846153846154\n",
      "val acc:  0.5344827586206896 train loss:  0.1529253465788705 train acc: 0.9907692307692307\n",
      "val acc:  0.603448275862069 train loss:  0.14712909857432047 train acc: 0.9907692307692307\n",
      "val acc:  0.603448275862069 train loss:  0.1519131940745172 train acc: 0.9876923076923076\n",
      "val acc:  0.603448275862069 train loss:  0.14777167921974546 train acc: 0.9907692307692307\n",
      "val acc:  0.5862068965517241 train loss:  0.1538722582516216 train acc: 0.9815384615384616\n",
      "val acc:  0.603448275862069 train loss:  0.15240503421851567 train acc: 0.9846153846153847\n",
      "val acc:  0.5689655172413793 train loss:  0.15958222427538463 train acc: 0.9815384615384616\n",
      "val acc:  0.5517241379310345 train loss:  0.27397693055016653 train acc: 0.9323076923076923\n",
      "val acc:  0.5517241379310345 train loss:  0.2196359857916832 train acc: 0.9661538461538461\n",
      "val acc:  0.5862068965517241 train loss:  0.1755387694353149 train acc: 0.9784615384615385\n",
      "val acc:  0.5689655172413793 train loss:  0.16160039000567936 train acc: 0.9784615384615385\n",
      "val acc:  0.5517241379310345 train loss:  0.1456372400834447 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.13967241311357134 train acc: 0.9876923076923076\n",
      "val acc:  0.5862068965517241 train loss:  0.13646740501835233 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.1300086790607089 train acc: 0.9907692307692307\n",
      "val acc:  0.5689655172413793 train loss:  0.131113235439573 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.12895507578338897 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.1427201706738699 train acc: 0.9876923076923076\n",
      "val acc:  0.5517241379310345 train loss:  0.14857912915093557 train acc: 0.9876923076923076\n",
      "val acc:  0.5344827586206896 train loss:  0.13207173560346877 train acc: 0.9907692307692307\n",
      "val acc:  0.5344827586206896 train loss:  0.1315741226786659 train acc: 0.9876923076923076\n",
      "val acc:  0.5689655172413793 train loss:  0.12627417487757547 train acc: 0.9907692307692307\n",
      "val acc:  0.5862068965517241 train loss:  0.12524474341244923 train acc: 0.9907692307692307\n",
      "val acc:  0.5517241379310345 train loss:  0.1250369550571555 train acc: 0.9907692307692307\n",
      "val acc:  0.603448275862069 train loss:  0.12366280598299843 train acc: 0.9907692307692307\n",
      "val acc:  0.5862068965517241 train loss:  0.13582384586334229 train acc: 0.9907692307692307\n",
      "val acc:  0.603448275862069 train loss:  0.12009134001675106 train acc: 0.9907692307692307\n",
      "val acc:  0.6206896551724138 train loss:  0.11935164814903623 train acc: 0.9907692307692307\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "crossEntropy = CrossEntropyLoss()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # train_feature_w2v, train_label_w2v = shuffle(train_feature_w2v, train_label_w2v)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    # random.shuffle(train_pred)\n",
    "    for feature, label in train_dataloader:\n",
    "        # print(feature.shape)\n",
    "        output = model(feature)\n",
    "        loss = crossEntropy(output.view((-1, 5)), label.view((-1,)))\n",
    "        correct += torch.sum(torch.argmax(output, dim = 1) == label).item()\n",
    "        all += feature.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "    train_acc = correct / all\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    model.eval()\n",
    "    for feature, label in val_dataloader:\n",
    "        all += feature.shape[0]\n",
    "        output = model(feature)\n",
    "        # print(torch.argmax(output, dim = 1), label)\n",
    "        correct += torch.sum(torch.argmax(output, dim = 1) == label).item()\n",
    "\n",
    "    print(\"val acc: \",correct / all, \"train loss: \", np.mean(loss_list), \"train acc:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 3], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_flattened = []\n",
    "dataset_pred_label = []\n",
    "\n",
    "dataset_pred_flattened_val = []\n",
    "dataset_pred_label_val = []\n",
    "\n",
    "for i, j in train_pred:\n",
    "    dataset_pred_flattened.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label.append(j.item())\n",
    "\n",
    "\n",
    "for i, j in val_pred:\n",
    "    dataset_pred_flattened_val.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label_val.append(j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3793103448275862"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf_perceptron = SGDClassifier(max_iter=1000)\n",
    "clf_perceptron.fit(dataset_pred_flattened, dataset_pred_label)\n",
    "\n",
    "accuracy_score(dataset_pred_label_val, clf_perceptron.predict(dataset_pred_flattened_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(output, dim = 1) == torch.tensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m Perceptron\n\u001b[0;32m      5\u001b[0m clf_perceptron \u001b[39m=\u001b[39m Perceptron()\n\u001b[1;32m----> 6\u001b[0m clf_perceptron\u001b[39m.\u001b[39mfit(train_feature, train_label)\n\u001b[0;32m      8\u001b[0m accuracy_score(test_label, clf_perceptron\u001b[39m.\u001b[39mpredict(test_feature))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_feature' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_perceptron = Perceptron()\n",
    "clf_perceptron.fit(train_feature, train_label)\n",
    "\n",
    "accuracy_score(test_label, clf_perceptron.predict(test_feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "566_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
