{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Token e4342ac4fcf98c2e1910b122cb4103c059f8bbfc',\n",
    "}\n",
    "\n",
    "response = requests.get('https://bilishorturl.ml/api/projects/3/export?exportType=JSON', headers=headers)\n",
    "\n",
    "import json\n",
    "annotations = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "keypoints_mapping = {}\n",
    "\n",
    "def getCenter(keypoints):\n",
    "    for point in keypoints:\n",
    "        point['center_x'] = point['x'] + point['width'] / 2 \n",
    "        point['center_y'] = point['y'] + point['height'] / 2\n",
    "\n",
    "# return_interpolation: When true append whether interpolated at the end\n",
    "# 1 means exist, 0 means missing\n",
    "def interpolation(keypoints, frames, return_interpolation):\n",
    "    prev = keypoints[0]['frame'] - 1\n",
    "    prev_x = 0\n",
    "    prev_y = 0\n",
    "    res = np.zeros((frames, 3 if return_interpolation else 2))\n",
    "    for i in keypoints:\n",
    "        diff = i['frame'] - prev\n",
    "        cur_x = i['center_x']\n",
    "        cur_y = i['center_y']\n",
    "        cur = i['frame']\n",
    "        for j in range(prev + 1, i['frame']):\n",
    "            # tmp = {'frame': j}\n",
    "            tmp_x = (prev_x * (cur - j) + cur_x * (j - prev)) / diff\n",
    "            tmp_y = (prev_y * (cur - j) + cur_y * (j - prev)) / diff\n",
    "\n",
    "            res[j - 1, :2] = (tmp_x / 100, tmp_y / 100)\n",
    "            if return_interpolation:\n",
    "                res[j - 1, -1] = 1\n",
    "            # tmp['interpolated'] = True\n",
    "            # res.append(tmp)\n",
    "        res[cur - 1, :2] = (cur_x / 100, cur_y / 100)\n",
    "        if return_interpolation:\n",
    "            res[cur - 1, -1] = 1\n",
    "        prev_x = cur_x\n",
    "        prev_y = cur_y\n",
    "        prev = i['frame']\n",
    "\n",
    "    return res\n",
    "\n",
    "def process_seq(boxes):\n",
    "    wand_end_keypoint = None\n",
    "    wand_tip_keypoint = None\n",
    "    wand_end_framesCount = None\n",
    "    wand_tip_framesCount = None\n",
    "\n",
    "    for i in boxes:\n",
    "        if 'labels' not in i['value'].keys():\n",
    "            continue\n",
    "        if i['value']['labels'][0] == labels_name[0]:\n",
    "            wand_tip_keypoint = i['value']['sequence']\n",
    "            wand_tip_framesCount = i['value']['framesCount']\n",
    "        elif i['value']['labels'][0] == labels_name[1]:\n",
    "            wand_end_keypoint = i['value']['sequence']\n",
    "            wand_end_framesCount = i['value']['framesCount']\n",
    "    \n",
    "    assert wand_tip_keypoint and wand_end_keypoint, f\"missing annotations for {annotation['id']}\"\n",
    "    assert wand_end_framesCount == wand_tip_framesCount, f'frames not matched for {annotation[\"id\"]}'\n",
    "\n",
    "    framesCount = wand_end_framesCount\n",
    "    # assert boxes[0]['value']['framesCount'] == boxes[1]['value']['framesCount'], f'frames not matched for {annotation[\"id\"]}'\n",
    "    # assert len(boxes) >= 2, f\"missing annotations for {annotation['id']}\"\n",
    "\n",
    "    \n",
    "    getCenter(wand_end_keypoint)\n",
    "\n",
    "    wand_end_keypoint = interpolation(wand_end_keypoint, framesCount, False)\n",
    "\n",
    "\n",
    "    getCenter(wand_tip_keypoint)\n",
    "\n",
    "    wand_tip_keypoint = interpolation(wand_tip_keypoint, framesCount, True)\n",
    "\n",
    "\n",
    "    return framesCount, wand_end_keypoint, wand_tip_keypoint\n",
    "\n",
    "\n",
    "labels_name = ['wand tip', 'wand end']\n",
    "\n",
    "for annotation in annotations:\n",
    "    vid_name = annotation['file_upload']\n",
    "\n",
    "    # boxes = annotation['annotations'][0]['result']\n",
    "    \n",
    "    framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(annotation['annotations'][0]['result'])\n",
    "\n",
    "    concat_keypoint = np.zeros(shape=(framesCount, 5))\n",
    "    concat_keypoint[:, :2] = wand_end_keypoint\n",
    "    concat_keypoint[:, 2:] = wand_tip_keypoint\n",
    "\n",
    "    \n",
    "    keypoints_mapping[vid_name] = torch.tensor(concat_keypoint)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/projects/3/tasks/?page_size=-1', headers=headers)\n",
    "\n",
    "all_tasks = json.loads(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/predictions', headers=headers)\n",
    "\n",
    "assert response.status_code == 200, \"connection error\"\n",
    "\n",
    "all_predictions = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'model_version', 'created_ago', 'result', 'score', 'cluster', 'neighbors', 'mislabeling', 'created_at', 'updated_at', 'task'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory that contains original videos\n",
    "\n",
    "import os\n",
    "source_dir = \"G:/.shortcut-targets-by-id/1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT/Data Wizards/dataset/processed_vid\"\n",
    "category_mapping = {\"3-24 V\": 0, \"3-25 bridge\": 1, \"3-25 R\": 2, \"Accio\": 1, \"Avada Kedavra\": 3, \"Invalid\": 4, \"Lumos\": 0, \"Revelio\": 2}\n",
    "\n",
    "vid_class = {} # name in processed_vid : category\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    tmp_root = root[root.rfind('/') + 1: ]\n",
    "    tmp_root = tmp_root[tmp_root.rfind('\\\\') + 1: ]\n",
    "    category = None if tmp_root not in category_mapping.keys() else category_mapping[tmp_root]\n",
    "    for name in files:\n",
    "        if not name.endswith('mp4'):\n",
    "            continue\n",
    "        assert category is not None, f\"No label at{os.path.join(root, name)} {tmp_root}\"\n",
    "\n",
    "        vid_class[name] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vid_class.pickle' ,'rb') as file:\n",
    "    vid_class = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMG_6458.mp4', 'IMG_6459_8.mp4', 'IMG_6458_2.mp4', 'IMG_6458_4.mp4', 'IMG_6458_6.mp4', 'IMG_6458_7.mp4', 'IMG_6458_5.mp4', 'IMG_6458_9.mp4', 'IMG_6458_8.mp4', 'IMG_6458_3.mp4', 'IMG_6458_11.mp4', 'IMG_6458_10.mp4', 'IMG_6458_12.mp4', 'IMG_6458_14.mp4', 'IMG_6458_13.mp4', 'IMG_6458_15.mp4', 'IMG_6458_16.mp4', 'IMG_6458_18.mp4', 'IMG_6458_19.mp4', 'IMG_6458_17.mp4', 'IMG_6459.mp4', 'IMG_6459_2.mp4', 'IMG_6459_3.mp4', 'IMG_6459_4.mp4', 'IMG_6459_5.mp4', 'IMG_6459_6.mp4', 'IMG_6459_7.mp4', 'IMG_6459_9.mp4', 'IMG_6459_10.mp4', 'IMG_6459_11.mp4', 'IMG_6460_9.mp4', 'IMG_6460_10.mp4', 'IMG_6460_11.mp4', 'IMG_6460_12.mp4', 'IMG_6460_13.mp4', 'IMG_6460_14.mp4', 'IMG_6460_15.mp4', 'IMG_6460_16.mp4', 'IMG_6460_17.mp4', 'IMG_6460_18.mp4', 'IMG_6460_19.mp4', 'IMG_6460_20.mp4', 'IMG_6460_21.mp4', 'IMG_6460_22.mp4', 'IMG_6460_23.mp4', 'IMG_6460_24.mp4', 'IMG_6460_25.mp4', 'IMG_6460_26.mp4', 'IMG_6460.mp4', 'IMG_6460_2.mp4', 'IMG_6460_3.mp4', 'IMG_6460_4.mp4', 'IMG_6460_5.mp4', 'IMG_6460_6.mp4', 'IMG_6460_7.mp4', 'IMG_6460_8.mp4', 'IMG_6408_37_.mp4', 'IMG_6408_39_.mp4', 'IMG_6408_34_.mp4', 'IMG_6408_35_.mp4', 'IMG_6408_41_.mp4', 'IMG_6408_33_.mp4', 'IMG_6408_36_.mp4', 'IMG_6408_38_.mp4', 'IMG_6408_40_.mp4', 'IMG_6408_32_.mp4', 'IMG_6408_31_.mp4', 'IMG_6408_30_.mp4', 'IMG_6408_28_.mp4', 'IMG_6408_29_.mp4', 'IMG_6408_24_.mp4', 'IMG_6408_27_.mp4', 'IMG_6408_26_.mp4', 'IMG_6408_25_.mp4', 'IMG_6408_20_.mp4', 'IMG_6408_22_.mp4', 'IMG_6408_21_.mp4', 'IMG_6408_23_.mp4', 'IMG_6408_19_.mp4', 'IMG_6408_14_.mp4', 'IMG_6408_15_.mp4', 'IMG_6408_18_.mp4', 'IMG_6408_17_.mp4', 'IMG_6408_16_.mp4', 'IMG_6408_13_.mp4', 'IMG_6408_11_.mp4', 'IMG_6408_12_.mp4', 'IMG_6408_10_.mp4', 'IMG_6408_9_.mp4', 'IMG_6408_7_.mp4', 'IMG_6408_8_.mp4', 'IMG_6408_5_.mp4', 'IMG_6408_6_.mp4', 'IMG_6408_3_.mp4', 'IMG_6408_1_.mp4', 'IMG_6408_4_.mp4', 'IMG_6408_2_.mp4', 'IMG_6408.mp4', 'IMG_1616.mp4', 'IMG_1607.mp4', 'IMG_1605.mp4', 'IMG_1612.mp4', 'IMG_1611.mp4', 'IMG_1613.mp4', 'IMG_1608.mp4', 'IMG_1614.mp4', 'IMG_1604.mp4', 'IMG_1609.mp4', 'IMG_1615.mp4', 'IMG_1610.mp4', '2023-03-26_04_54_10.mp4', '2023-03-26_04_54_03.mp4', '2023-03-26_04_53_56.mp4', '2023-03-26_04_53_42.mp4', '2023-03-26_04_53_49.mp4', '2023-03-26_04_59_23.mp4', '2023-03-26_04_59_09.mp4', '2023-03-26_04_59_16.mp4', '2023-03-26_04_59_02.mp4', '2023-03-26_04_58_48.mp4', '2023-03-26_04_58_55.mp4', '2023-03-26_04_58_41.mp4', '2023-03-26_04_58_26.mp4', '2023-03-26_04_58_34.mp4', '2023-03-26_04_58_19.mp4', '2023-03-26_04_57_03.mp4', '2023-03-26_04_56_49.mp4', '2023-03-26_04_56_41.mp4', '2023-03-26_04_56_56.mp4', '2023-03-26_04_56_34.mp4', '2023-03-26_04_56_27.mp4', '2023-03-26_04_56_20.mp4', '2023-03-26_04_56_13.mp4', '2023-03-26_04_56_06.mp4', '2023-03-26_04_54_46.mp4', '2023-03-26_04_54_39.mp4', '2023-03-26_04_54_32.mp4', '2023-03-26_04_54_18.mp4', '2023-03-26_04_54_25.mp4', '2023-03-26_04_55_59.mp4', '04-06-2023-1.mp4', '04-06-2023-2.mp4', '04-06-2023-28.mp4', '04-06-2023-26.mp4', '04-06-2023-24.mp4', '04-06-2023-22.mp4', '04-06-2023-21.mp4', '04-06-2023-20.mp4', '04-06-2023-19.mp4', '04-06-2023-18.mp4', '04-06-2023-17.mp4', '04-06-2023-16.mp4', '04-06-2023-14.mp4', '04-06-2023-13.mp4', '04-06-2023-10.mp4', '04-06-2023-9.mp4', '04-06-2023-8.mp4', '04-06-2023-7.mp4', '04-06-2023-6.mp4', '04-06-2023-5.mp4', '04-06-2023-29.mp4', '04-06-2023-27.mp4', '04-06-2023-4.mp4', '04-06-2023-23.mp4', '04-06-2023-25.mp4', '04-06-2023-15.mp4', '04-06-2023-11.mp4', '04-06-2023-12.mp4', '04-06-2023-3.mp4', '04-06-2023-36.mp4', '04-06-2023-35.mp4', '04-06-2023-34.mp4', '04-06-2023-33.mp4', '04-06-2023-32.mp4', '04-06-2023-31.mp4', '04-06-2023-30.mp4', 'IMG_1631.mp4', 'IMG_1632.mp4', 'IMG_1635.mp4', 'IMG_1634.mp4', 'IMG_1626.mp4', 'IMG_1627.mp4', 'IMG_1633.mp4', 'IMG_1628.mp4', 'IMG_1630.mp4', 'IMG_1629.mp4', '2023-03-26_05_12_55.mp4', '2023-03-26_05_13_02.mp4', '2023-03-26_05_12_48.mp4', '2023-03-26_05_12_41.mp4', '2023-03-26_05_12_19.mp4', '2023-03-26_05_12_34.mp4', '2023-03-26_05_12_27.mp4', '2023-03-26_05_12_12.mp4', '2023-03-26_05_12_05.mp4', '2023-03-26_05_11_58.mp4', '2023-03-26_05_11_36.mp4', '2023-03-26_05_11_51.mp4', '2023-03-26_05_11_44.mp4', '2023-03-26_05_11_29.mp4', '2023-03-26_05_11_22.mp4', '2023-03-26_05_11_15.mp4', '2023-03-26_05_11_08.mp4', '2023-03-26_05_11_01.mp4', '2023-03-26_05_10_54.mp4', '2023-03-26_05_10_46.mp4', '2023-03-26_05_08_43.mp4', '2023-03-26_05_08_36.mp4', '2023-03-26_05_08_29.mp4', '2023-03-26_05_08_21.mp4', '2023-03-26_05_08_00.mp4', '2023-03-26_05_07_53.mp4', '2023-03-26_05_07_46.mp4', '2023-03-26_05_07_38.mp4', 'IMG_1639.mp4', 'IMG_1640.mp4', 'IMG_1643.mp4', 'IMG_1644.mp4', 'IMG_1646.mp4', 'IMG_1638.mp4', 'IMG_1641.mp4', 'IMG_1645.mp4', 'IMG_1647.mp4', 'IMG_1642.mp4', '2023-03-26_05_24_29.mp4', '2023-03-26_05_24_58.mp4', '2023-03-26_05_24_36.mp4', '2023-03-26_05_24_22.mp4', '2023-03-26_05_24_50.mp4', '2023-03-26_05_24_43.mp4', '2023-03-26_05_24_15.mp4', '2023-03-26_05_24_08.mp4', '2023-03-26_05_24_01.mp4', '2023-03-26_05_23_53.mp4', '2023-03-26_05_23_32.mp4', '2023-03-26_05_23_39.mp4', '2023-03-26_05_23_46.mp4', '2023-03-26_05_23_25.mp4', '2023-03-26_05_23_11.mp4', '2023-03-26_05_23_18.mp4', '2023-03-26_05_23_04.mp4', '2023-03-26_05_22_56.mp4', '2023-03-26_05_22_49.mp4', '2023-03-26_05_22_42.mp4', '2023-03-26_05_26_09.mp4', '2023-03-26_05_26_02.mp4', '2023-03-26_05_25_55.mp4', '2023-03-26_05_25_47.mp4', '2023-03-26_05_25_40.mp4', '2023-03-26_05_25_33.mp4', '2023-03-26_05_25_26.mp4', '2023-03-26_05_25_19.mp4', '2023-03-26_05_25_12.mp4', '2023-03-26_05_25_05.mp4', 'IMG_1661.mp4', 'IMG_1659.mp4', 'IMG_1653.mp4', 'IMG_1649.mp4', 'IMG_1657.mp4', 'IMG_1655.mp4', 'IMG_1660.mp4', 'IMG_1654.mp4', 'IMG_1658.mp4', 'IMG_1651.mp4', 'IMG_1648.mp4', 'IMG_1652.mp4', 'IMG_1656.mp4', 'IMG_1650.mp4', '2023-03-26_05_21_09.mp4', '2023-03-26_05_21_02.mp4', '2023-03-26_05_20_54.mp4', '2023-03-26_05_20_47.mp4', '2023-03-26_05_20_40.mp4', '2023-03-26_05_20_33.mp4', '2023-03-26_05_20_26.mp4', '2023-03-26_05_20_19.mp4', '2023-03-26_05_20_12.mp4', '2023-03-26_05_20_05.mp4', '2023-03-26_05_18_04.mp4', '2023-03-26_05_17_57.mp4', '2023-03-26_05_17_50.mp4', '2023-03-26_05_17_43.mp4', '2023-03-26_05_17_36.mp4', '2023-03-26_05_17_29.mp4', '2023-03-26_05_17_22.mp4', '2023-03-26_05_17_14.mp4', '2023-03-26_05_17_00.mp4', '2023-03-26_05_17_07.mp4', '2023-03-26_05_16_53.mp4', '2023-03-26_05_16_32.mp4', '2023-03-26_05_16_46.mp4', '2023-03-26_05_16_25.mp4', '2023-03-26_05_16_39.mp4', '2023-03-26_05_16_17.mp4', '2023-03-26_05_16_10.mp4', '2023-03-26_05_16_03.mp4', '2023-03-26_05_15_56.mp4', '2023-03-26_05_15_49.mp4', '04-06-2023-40.mp4', '04-06-2023-39.mp4', '04-06-2023-38.mp4', '04-06-2023-37.mp4', 'IMG_1618.mp4', '2023-03-26_05_31_19.mp4', '2023-03-26_05_31_12.mp4', '2023-03-26_05_31_05.mp4', '2023-03-26_05_30_58.mp4', '2023-03-26_05_30_51.mp4', '2023-03-26_05_30_44.mp4', '2023-03-26_05_30_30.mp4', '2023-03-26_05_30_37.mp4', '2023-03-26_05_30_15.mp4', '2023-03-26_05_30_22.mp4', '2023-03-26_05_30_08.mp4', '2023-03-26_05_29_54.mp4', '2023-03-26_05_30_01.mp4', '2023-03-26_05_29_47.mp4', '2023-03-26_05_29_40.mp4', '2023-03-26_05_28_27.mp4', '2023-03-26_05_28_20.mp4', '2023-03-26_05_28_13.mp4', '2023-03-26_05_27_58.mp4', '2023-03-26_05_28_06.mp4', '2023-03-26_05_27_51.mp4', '2023-03-26_05_27_37.mp4', '2023-03-26_05_27_44.mp4', '2023-03-26_05_27_30.mp4', '2023-03-26_05_27_23.mp4'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_class.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_seq(j, pad_length = 90):\n",
    "    res = torch.zeros(size=(pad_length, j.shape[1]))\n",
    "    padded = pad_length - j.shape[0]\n",
    "    res[int(padded / 2): int(padded / 2) + j.shape[0],:] = j\n",
    "    return res\n",
    "\n",
    "\n",
    "# keypoints_mapping_padded = {}\n",
    "dataset = []\n",
    "\n",
    "for file_name, j in keypoints_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    dataset.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long)))\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "d = int(len(dataset) * 0.85)\n",
    "\n",
    "\n",
    "train = dataset[:d]\n",
    "val = dataset[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./runs/detect/train5/weights/best.pt\"\n",
    "\n",
    "keypoints_pred_mapping = {}\n",
    "\n",
    "for i in all_tasks:\n",
    "    file_name = i['data']['video']\n",
    "    file_name = file_name[file_name.rfind('/') + 1:]\n",
    "    preds = i['predictions']\n",
    "    for j in preds[::-1]:\n",
    "        if j['model_version'] == model_name:\n",
    "            framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(j['result'])\n",
    "            concat_keypoint = np.zeros(shape=(framesCount, 5))\n",
    "            concat_keypoint[:, :2] = wand_end_keypoint\n",
    "            concat_keypoint[:, 2:] = wand_tip_keypoint\n",
    "\n",
    "            keypoints_pred_mapping[file_name] = torch.tensor(concat_keypoint)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = []\n",
    "missing = 0\n",
    "\n",
    "\n",
    "for file_name, j in keypoints_pred_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    if file_name in vid_class.keys():\n",
    "        dataset_pred.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long)))\n",
    "    # else:\n",
    "    #     missing += 1\n",
    "    #     print(file_name)\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset_pred)\n",
    "d = int(len(dataset_pred) * 0.85)\n",
    "\n",
    "\n",
    "train_pred = dataset_pred[:d]\n",
    "val_pred = dataset_pred[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size = 5, hidden_size = 100, batch_first = True, bidirectional = True, dropout = 0.1, num_layers = 2)\n",
    "        self.fc = nn.Linear(200, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.rnn(x)[1][0].shape)\n",
    "        # return self.fc(torch.squeeze(self.rnn(x)[1][0], dim=0))\n",
    "        # print(self.rnn(x)[0].shape)\n",
    "        return self.fc(self.rnn(x)[0][:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "device = 'cuda'\n",
    "\n",
    "model = lstm()\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74, 56, 55, 59, 81]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = [0] * 5\n",
    "for feature, label in train_pred:\n",
    "    cnts[label.item()] += 1\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2414)\n",
      "tensor(0.2069)\n",
      "tensor(0.2069)\n",
      "tensor(0.2759)\n",
      "tensor(0.2759)\n",
      "tensor(0.2931)\n",
      "tensor(0.3103)\n",
      "tensor(0.2069)\n",
      "tensor(0.2586)\n",
      "tensor(0.1897)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "crossEntropy = CrossEntropyLoss()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # train_feature_w2v, train_label_w2v = shuffle(train_feature_w2v, train_label_w2v)\n",
    "    model.train()\n",
    "    random.shuffle(train_pred)\n",
    "    for feature, label in train_pred:\n",
    "        output = model(feature.unsqueeze(0))\n",
    "        loss = crossEntropy(output.view((-1, 5)), label.view((-1,)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    model.eval()\n",
    "    for feature, label in val_pred:\n",
    "        all += 1\n",
    "        output = model(feature.unsqueeze(0))\n",
    "        # print(torch.argmax(output, dim = 1), label)\n",
    "        correct += torch.sum(torch.argmax(output, dim = 1) == label)\n",
    "\n",
    "    print(correct / all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_flattened = []\n",
    "dataset_pred_label = []\n",
    "\n",
    "dataset_pred_flattened_val = []\n",
    "dataset_pred_label_val = []\n",
    "\n",
    "for i, j in train_pred:\n",
    "    dataset_pred_flattened.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label.append(j.item())\n",
    "\n",
    "\n",
    "for i, j in val_pred:\n",
    "    dataset_pred_flattened_val.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label_val.append(j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41379310344827586"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf_perceptron = SGDClassifier(max_iter=1000)\n",
    "clf_perceptron.fit(dataset_pred_flattened, dataset_pred_label)\n",
    "\n",
    "accuracy_score(dataset_pred_label_val, clf_perceptron.predict(dataset_pred_flattened_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.argmax(output, dim = 1) == torch.tensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_perceptron = Perceptron()\n",
    "clf_perceptron.fit(train_feature, train_label)\n",
    "\n",
    "accuracy_score(test_label, clf_perceptron.predict(test_feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "566_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
