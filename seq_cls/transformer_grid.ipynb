{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Token e4342ac4fcf98c2e1910b122cb4103c059f8bbfc',\n",
    "}\n",
    "\n",
    "response = requests.get('https://bilishorturl.ml/api/projects/3/export?exportType=JSON', headers=headers)\n",
    "\n",
    "import json\n",
    "annotations = json.loads(response.content)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "keypoints_mapping = {}\n",
    "\n",
    "x_grid_size = 9\n",
    "y_grid_size = 16\n",
    "grid_num = x_grid_size * y_grid_size\n",
    "\n",
    "unknown_idx = 0\n",
    "\n",
    "\n",
    "def getCenter(keypoints):\n",
    "    for point in keypoints:\n",
    "        point['center_x'] = point['x'] + point['width'] / 2 \n",
    "        point['center_y'] = point['y'] + point['height'] / 2\n",
    "\n",
    "# return_interpolation: When true append whether interpolated at the end\n",
    "# 1 means exist, 0 means missing\n",
    "def interpolation(keypoints, frames):\n",
    "    prev = keypoints[0]['frame'] - 1\n",
    "    prev_x = 0\n",
    "    prev_y = 0\n",
    "    res = np.zeros((frames,1))\n",
    "    for i in keypoints:\n",
    "        diff = i['frame'] - prev\n",
    "        cur_x = i['center_x']\n",
    "        cur_y = i['center_y']\n",
    "        cur = i['frame']\n",
    "        for j in range(prev + 1, i['frame']):\n",
    "            # tmp = {'frame': j}\n",
    "            tmp_x = (prev_x * (cur - j) + cur_x * (j - prev)) / diff\n",
    "            tmp_y = (prev_y * (cur - j) + cur_y * (j - prev)) / diff\n",
    "\n",
    "            res[j - 1, 0] = min(int(tmp_x / 100 * x_grid_size) * y_grid_size + int(tmp_y / 100 * y_grid_size) + 1, grid_num)\n",
    "            # if return_interpolation:\n",
    "            #     res[j - 1, -1] = 1\n",
    "            # tmp['interpolated'] = True\n",
    "            # res.append(tmp)\n",
    "        res[cur - 1, 0] = min(int(cur_x / 100 * x_grid_size) * y_grid_size + int(cur_y / 100 * y_grid_size) + 1, grid_num)\n",
    "        # if return_interpolation:\n",
    "        #     res[cur - 1, -1] = 1\n",
    "        prev_x = cur_x\n",
    "        prev_y = cur_y\n",
    "        prev = i['frame']\n",
    "\n",
    "    return res\n",
    "\n",
    "def process_seq(boxes):\n",
    "    wand_end_keypoint = None\n",
    "    wand_tip_keypoint = None\n",
    "    wand_end_framesCount = None\n",
    "    wand_tip_framesCount = None\n",
    "\n",
    "    for i in boxes:\n",
    "        if 'labels' not in i['value'].keys():\n",
    "            continue\n",
    "        if i['value']['labels'][0] == labels_name[0]:\n",
    "            wand_tip_keypoint = i['value']['sequence']\n",
    "            wand_tip_framesCount = i['value']['framesCount']\n",
    "        elif i['value']['labels'][0] == labels_name[1]:\n",
    "            wand_end_keypoint = i['value']['sequence']\n",
    "            wand_end_framesCount = i['value']['framesCount']\n",
    "    \n",
    "    assert wand_tip_keypoint and wand_end_keypoint, f\"missing annotations for {annotation['id']}\"\n",
    "    assert wand_end_framesCount == wand_tip_framesCount, f'frames not matched for {annotation[\"id\"]}'\n",
    "\n",
    "    framesCount = wand_end_framesCount\n",
    "    # assert boxes[0]['value']['framesCount'] == boxes[1]['value']['framesCount'], f'frames not matched for {annotation[\"id\"]}'\n",
    "    # assert len(boxes) >= 2, f\"missing annotations for {annotation['id']}\"\n",
    "\n",
    "    \n",
    "    getCenter(wand_end_keypoint)\n",
    "\n",
    "    wand_end_keypoint = interpolation(wand_end_keypoint, framesCount)\n",
    "\n",
    "\n",
    "    getCenter(wand_tip_keypoint)\n",
    "\n",
    "    wand_tip_keypoint = interpolation(wand_tip_keypoint, framesCount)\n",
    "\n",
    "\n",
    "    return framesCount, wand_end_keypoint, wand_tip_keypoint\n",
    "\n",
    "\n",
    "labels_name = ['wand tip', 'wand end']\n",
    "\n",
    "for annotation in annotations:\n",
    "    vid_name = annotation['file_upload']\n",
    "\n",
    "    # boxes = annotation['annotations'][0]['result']\n",
    "    \n",
    "    framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(annotation['annotations'][0]['result'])\n",
    "\n",
    "    concat_keypoint = np.zeros(shape=(framesCount, 2))\n",
    "    concat_keypoint[:, :1] = wand_end_keypoint\n",
    "    concat_keypoint[:, 1:] = wand_tip_keypoint\n",
    "\n",
    "    \n",
    "    keypoints_mapping[vid_name] = torch.tensor(concat_keypoint)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/projects/3/tasks/?page_size=-1', headers=headers)\n",
    "\n",
    "all_tasks = json.loads(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'https://bilishorturl.ml/api/predictions', headers=headers)\n",
    "\n",
    "assert response.status_code == 200, \"connection error\"\n",
    "\n",
    "all_predictions = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'model_version', 'created_ago', 'result', 'score', 'cluster', 'neighbors', 'mislabeling', 'created_at', 'updated_at', 'task'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory that contains original videos\n",
    "\n",
    "import os\n",
    "source_dir = \"G:/.shortcut-targets-by-id/1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT/Data Wizards/dataset/processed_vid\"\n",
    "category_mapping = {\"3-24 V\": 0, \"3-25 bridge\": 1, \"3-25 R\": 2, \"Accio\": 1, \"Avada Kedavra\": 3, \"Invalid\": 4, \"Lumos\": 0, \"Revelio\": 2}\n",
    "\n",
    "vid_class = {} # name in processed_vid : category\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    tmp_root = root[root.rfind('/') + 1: ]\n",
    "    tmp_root = tmp_root[tmp_root.rfind('\\\\') + 1: ]\n",
    "    category = None if tmp_root not in category_mapping.keys() else category_mapping[tmp_root]\n",
    "    for name in files:\n",
    "        if not name.endswith('mp4'):\n",
    "            continue\n",
    "        assert category is not None, f\"No label at{os.path.join(root, name)} {tmp_root}\"\n",
    "\n",
    "        vid_class[name] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMG_6458.mp4', 'IMG_6459_8.mp4', 'IMG_6458_2.mp4', 'IMG_6458_4.mp4', 'IMG_6458_6.mp4', 'IMG_6458_7.mp4', 'IMG_6458_5.mp4', 'IMG_6458_9.mp4', 'IMG_6458_8.mp4', 'IMG_6458_3.mp4', 'IMG_6458_11.mp4', 'IMG_6458_10.mp4', 'IMG_6458_12.mp4', 'IMG_6458_14.mp4', 'IMG_6458_13.mp4', 'IMG_6458_15.mp4', 'IMG_6458_16.mp4', 'IMG_6458_18.mp4', 'IMG_6458_19.mp4', 'IMG_6458_17.mp4', 'IMG_6459.mp4', 'IMG_6459_2.mp4', 'IMG_6459_3.mp4', 'IMG_6459_4.mp4', 'IMG_6459_5.mp4', 'IMG_6459_6.mp4', 'IMG_6459_7.mp4', 'IMG_6459_9.mp4', 'IMG_6459_10.mp4', 'IMG_6459_11.mp4', 'IMG_6460_9.mp4', 'IMG_6460_10.mp4', 'IMG_6460_11.mp4', 'IMG_6460_12.mp4', 'IMG_6460_13.mp4', 'IMG_6460_14.mp4', 'IMG_6460_15.mp4', 'IMG_6460_16.mp4', 'IMG_6460_17.mp4', 'IMG_6460_18.mp4', 'IMG_6460_19.mp4', 'IMG_6460_20.mp4', 'IMG_6460_21.mp4', 'IMG_6460_22.mp4', 'IMG_6460_23.mp4', 'IMG_6460_24.mp4', 'IMG_6460_25.mp4', 'IMG_6460_26.mp4', 'IMG_6460.mp4', 'IMG_6460_2.mp4', 'IMG_6460_3.mp4', 'IMG_6460_4.mp4', 'IMG_6460_5.mp4', 'IMG_6460_6.mp4', 'IMG_6460_7.mp4', 'IMG_6460_8.mp4', 'IMG_6408_37_.mp4', 'IMG_6408_39_.mp4', 'IMG_6408_34_.mp4', 'IMG_6408_35_.mp4', 'IMG_6408_41_.mp4', 'IMG_6408_33_.mp4', 'IMG_6408_36_.mp4', 'IMG_6408_38_.mp4', 'IMG_6408_40_.mp4', 'IMG_6408_32_.mp4', 'IMG_6408_31_.mp4', 'IMG_6408_30_.mp4', 'IMG_6408_28_.mp4', 'IMG_6408_29_.mp4', 'IMG_6408_24_.mp4', 'IMG_6408_27_.mp4', 'IMG_6408_26_.mp4', 'IMG_6408_25_.mp4', 'IMG_6408_20_.mp4', 'IMG_6408_22_.mp4', 'IMG_6408_21_.mp4', 'IMG_6408_23_.mp4', 'IMG_6408_19_.mp4', 'IMG_6408_14_.mp4', 'IMG_6408_15_.mp4', 'IMG_6408_18_.mp4', 'IMG_6408_17_.mp4', 'IMG_6408_16_.mp4', 'IMG_6408_13_.mp4', 'IMG_6408_11_.mp4', 'IMG_6408_12_.mp4', 'IMG_6408_10_.mp4', 'IMG_6408_9_.mp4', 'IMG_6408_7_.mp4', 'IMG_6408_8_.mp4', 'IMG_6408_5_.mp4', 'IMG_6408_6_.mp4', 'IMG_6408_3_.mp4', 'IMG_6408_1_.mp4', 'IMG_6408_4_.mp4', 'IMG_6408_2_.mp4', 'IMG_6408.mp4', 'IMG_1616.mp4', 'IMG_1607.mp4', 'IMG_1605.mp4', 'IMG_1612.mp4', 'IMG_1611.mp4', 'IMG_1613.mp4', 'IMG_1608.mp4', 'IMG_1614.mp4', 'IMG_1604.mp4', 'IMG_1609.mp4', 'IMG_1615.mp4', 'IMG_1610.mp4', '2023-03-26_04_54_10.mp4', '2023-03-26_04_54_03.mp4', '2023-03-26_04_53_56.mp4', '2023-03-26_04_53_42.mp4', '2023-03-26_04_53_49.mp4', '2023-03-26_04_59_23.mp4', '2023-03-26_04_59_09.mp4', '2023-03-26_04_59_16.mp4', '2023-03-26_04_59_02.mp4', '2023-03-26_04_58_48.mp4', '2023-03-26_04_58_55.mp4', '2023-03-26_04_58_41.mp4', '2023-03-26_04_58_26.mp4', '2023-03-26_04_58_34.mp4', '2023-03-26_04_58_19.mp4', '2023-03-26_04_57_03.mp4', '2023-03-26_04_56_49.mp4', '2023-03-26_04_56_41.mp4', '2023-03-26_04_56_56.mp4', '2023-03-26_04_56_34.mp4', '2023-03-26_04_56_27.mp4', '2023-03-26_04_56_20.mp4', '2023-03-26_04_56_13.mp4', '2023-03-26_04_56_06.mp4', '2023-03-26_04_54_46.mp4', '2023-03-26_04_54_39.mp4', '2023-03-26_04_54_32.mp4', '2023-03-26_04_54_18.mp4', '2023-03-26_04_54_25.mp4', '2023-03-26_04_55_59.mp4', '04-06-2023-1.mp4', '04-06-2023-2.mp4', '04-06-2023-28.mp4', '04-06-2023-26.mp4', '04-06-2023-24.mp4', '04-06-2023-22.mp4', '04-06-2023-21.mp4', '04-06-2023-20.mp4', '04-06-2023-19.mp4', '04-06-2023-18.mp4', '04-06-2023-17.mp4', '04-06-2023-16.mp4', '04-06-2023-14.mp4', '04-06-2023-13.mp4', '04-06-2023-10.mp4', '04-06-2023-9.mp4', '04-06-2023-8.mp4', '04-06-2023-7.mp4', '04-06-2023-6.mp4', '04-06-2023-5.mp4', '04-06-2023-29.mp4', '04-06-2023-27.mp4', '04-06-2023-4.mp4', '04-06-2023-23.mp4', '04-06-2023-25.mp4', '04-06-2023-15.mp4', '04-06-2023-11.mp4', '04-06-2023-12.mp4', '04-06-2023-3.mp4', '04-06-2023-36.mp4', '04-06-2023-35.mp4', '04-06-2023-34.mp4', '04-06-2023-33.mp4', '04-06-2023-32.mp4', '04-06-2023-31.mp4', '04-06-2023-30.mp4', 'IMG_1631.mp4', 'IMG_1632.mp4', 'IMG_1635.mp4', 'IMG_1634.mp4', 'IMG_1626.mp4', 'IMG_1627.mp4', 'IMG_1633.mp4', 'IMG_1628.mp4', 'IMG_1630.mp4', 'IMG_1629.mp4', '2023-03-26_05_12_55.mp4', '2023-03-26_05_13_02.mp4', '2023-03-26_05_12_48.mp4', '2023-03-26_05_12_41.mp4', '2023-03-26_05_12_19.mp4', '2023-03-26_05_12_34.mp4', '2023-03-26_05_12_27.mp4', '2023-03-26_05_12_12.mp4', '2023-03-26_05_12_05.mp4', '2023-03-26_05_11_58.mp4', '2023-03-26_05_11_36.mp4', '2023-03-26_05_11_51.mp4', '2023-03-26_05_11_44.mp4', '2023-03-26_05_11_29.mp4', '2023-03-26_05_11_22.mp4', '2023-03-26_05_11_15.mp4', '2023-03-26_05_11_08.mp4', '2023-03-26_05_11_01.mp4', '2023-03-26_05_10_54.mp4', '2023-03-26_05_10_46.mp4', '2023-03-26_05_08_43.mp4', '2023-03-26_05_08_36.mp4', '2023-03-26_05_08_29.mp4', '2023-03-26_05_08_21.mp4', '2023-03-26_05_08_00.mp4', '2023-03-26_05_07_53.mp4', '2023-03-26_05_07_46.mp4', '2023-03-26_05_07_38.mp4', 'IMG_1639.mp4', 'IMG_1640.mp4', 'IMG_1643.mp4', 'IMG_1644.mp4', 'IMG_1646.mp4', 'IMG_1638.mp4', 'IMG_1641.mp4', 'IMG_1645.mp4', 'IMG_1647.mp4', 'IMG_1642.mp4', '2023-03-26_05_24_29.mp4', '2023-03-26_05_24_58.mp4', '2023-03-26_05_24_36.mp4', '2023-03-26_05_24_22.mp4', '2023-03-26_05_24_50.mp4', '2023-03-26_05_24_43.mp4', '2023-03-26_05_24_15.mp4', '2023-03-26_05_24_08.mp4', '2023-03-26_05_24_01.mp4', '2023-03-26_05_23_53.mp4', '2023-03-26_05_23_32.mp4', '2023-03-26_05_23_39.mp4', '2023-03-26_05_23_46.mp4', '2023-03-26_05_23_25.mp4', '2023-03-26_05_23_11.mp4', '2023-03-26_05_23_18.mp4', '2023-03-26_05_23_04.mp4', '2023-03-26_05_22_56.mp4', '2023-03-26_05_22_49.mp4', '2023-03-26_05_22_42.mp4', '2023-03-26_05_26_09.mp4', '2023-03-26_05_26_02.mp4', '2023-03-26_05_25_55.mp4', '2023-03-26_05_25_47.mp4', '2023-03-26_05_25_40.mp4', '2023-03-26_05_25_33.mp4', '2023-03-26_05_25_26.mp4', '2023-03-26_05_25_19.mp4', '2023-03-26_05_25_12.mp4', '2023-03-26_05_25_05.mp4', 'IMG_1661.mp4', 'IMG_1659.mp4', 'IMG_1653.mp4', 'IMG_1649.mp4', 'IMG_1657.mp4', 'IMG_1655.mp4', 'IMG_1660.mp4', 'IMG_1654.mp4', 'IMG_1658.mp4', 'IMG_1651.mp4', 'IMG_1648.mp4', 'IMG_1652.mp4', 'IMG_1656.mp4', 'IMG_1650.mp4', '2023-03-26_05_21_09.mp4', '2023-03-26_05_21_02.mp4', '2023-03-26_05_20_54.mp4', '2023-03-26_05_20_47.mp4', '2023-03-26_05_20_40.mp4', '2023-03-26_05_20_33.mp4', '2023-03-26_05_20_26.mp4', '2023-03-26_05_20_19.mp4', '2023-03-26_05_20_12.mp4', '2023-03-26_05_20_05.mp4', '2023-03-26_05_18_04.mp4', '2023-03-26_05_17_57.mp4', '2023-03-26_05_17_50.mp4', '2023-03-26_05_17_43.mp4', '2023-03-26_05_17_36.mp4', '2023-03-26_05_17_29.mp4', '2023-03-26_05_17_22.mp4', '2023-03-26_05_17_14.mp4', '2023-03-26_05_17_00.mp4', '2023-03-26_05_17_07.mp4', '2023-03-26_05_16_53.mp4', '2023-03-26_05_16_32.mp4', '2023-03-26_05_16_46.mp4', '2023-03-26_05_16_25.mp4', '2023-03-26_05_16_39.mp4', '2023-03-26_05_16_17.mp4', '2023-03-26_05_16_10.mp4', '2023-03-26_05_16_03.mp4', '2023-03-26_05_15_56.mp4', '2023-03-26_05_15_49.mp4', '04-06-2023-40.mp4', '04-06-2023-39.mp4', '04-06-2023-38.mp4', '04-06-2023-37.mp4', 'IMG_1618.mp4', '2023-03-26_05_31_19.mp4', '2023-03-26_05_31_12.mp4', '2023-03-26_05_31_05.mp4', '2023-03-26_05_30_58.mp4', '2023-03-26_05_30_51.mp4', '2023-03-26_05_30_44.mp4', '2023-03-26_05_30_30.mp4', '2023-03-26_05_30_37.mp4', '2023-03-26_05_30_15.mp4', '2023-03-26_05_30_22.mp4', '2023-03-26_05_30_08.mp4', '2023-03-26_05_29_54.mp4', '2023-03-26_05_30_01.mp4', '2023-03-26_05_29_47.mp4', '2023-03-26_05_29_40.mp4', '2023-03-26_05_28_27.mp4', '2023-03-26_05_28_20.mp4', '2023-03-26_05_28_13.mp4', '2023-03-26_05_27_58.mp4', '2023-03-26_05_28_06.mp4', '2023-03-26_05_27_51.mp4', '2023-03-26_05_27_37.mp4', '2023-03-26_05_27_44.mp4', '2023-03-26_05_27_30.mp4', '2023-03-26_05_27_23.mp4'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_class.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_seq(j, pad_length = 90, device = device):\n",
    "    res = torch.zeros(size=(pad_length, j.shape[1]), dtype=torch.long)\n",
    "    padded = pad_length - j.shape[0]\n",
    "    res[int(padded / 2): int(padded / 2) + j.shape[0],:] = j\n",
    "    return res.to(device)\n",
    "\n",
    "\n",
    "# keypoints_mapping_padded = {}\n",
    "dataset = []\n",
    "\n",
    "for file_name, j in keypoints_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    dataset.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long, device = device)))\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "d = int(len(dataset) * 0.85)\n",
    "\n",
    "\n",
    "train = dataset[:d]\n",
    "val = dataset[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./runs/detect/train5/weights/best.pt\"\n",
    "\n",
    "keypoints_pred_mapping = {}\n",
    "\n",
    "for i in all_tasks:\n",
    "    file_name = i['data']['video']\n",
    "    file_name = file_name[file_name.rfind('/') + 1:]\n",
    "    preds = i['predictions']\n",
    "    for j in preds[::-1]:\n",
    "        if j['model_version'] == model_name:\n",
    "            framesCount, wand_end_keypoint, wand_tip_keypoint = process_seq(j['result'])\n",
    "            concat_keypoint = np.zeros(shape=(framesCount, 2))\n",
    "            concat_keypoint[:, :1] = wand_end_keypoint\n",
    "            concat_keypoint[:, 1:] = wand_tip_keypoint\n",
    "\n",
    "            keypoints_pred_mapping[file_name] = torch.tensor(concat_keypoint, dtype=torch.long)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = []\n",
    "missing = 0\n",
    "\n",
    "\n",
    "for file_name, j in keypoints_pred_mapping.items():\n",
    "    file_name = file_name[file_name.find('-') + 1:]\n",
    "    if file_name in vid_class.keys():\n",
    "        dataset_pred.append((pad_seq(j), torch.tensor(vid_class[file_name], dtype = torch.long, device=device)))\n",
    "    # else:\n",
    "    #     missing += 1\n",
    "    #     print(file_name)\n",
    "    # print(j.shape, vid_class[file_name])\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset_pred)\n",
    "d = int(len(dataset_pred) * 0.85)\n",
    "\n",
    "\n",
    "train_pred = dataset_pred[:d]\n",
    "val_pred = dataset_pred[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]\n",
    "    \n",
    "train_dataset = ClsDataset(train_pred)\n",
    "val_dataset = ClsDataset(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "config = BertConfig('prajjwal1/bert-tiny')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 96,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 384,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 180,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 145\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size = x_grid_size * y_grid_size + 1\n",
    "config.hidden_size = 96\n",
    "config.intermediate_size = 4 * config.hidden_size\n",
    "config.max_position_embeddings = 180\n",
    "config.num_labels = 5\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat((torch.zeros(size=(1,100)), torch.zeros(size=(1,100))), dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model, 0.1), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 21, 30, 11, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = [0] * 5\n",
    "for feature, label in train:\n",
    "    cnts[label.item()] += 1\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 4, 6, 2, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = [0] * 5\n",
    "for feature, label in val:\n",
    "    cnts[label.item()] += 1\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.3620689655172414 train loss:  1.60845795131865 train acc: 0.2123076923076923\n",
      "val acc:  0.41379310344827586 train loss:  1.5890981015704928 train acc: 0.28923076923076924\n",
      "val acc:  0.41379310344827586 train loss:  1.565679362842015 train acc: 0.39076923076923076\n",
      "val acc:  0.41379310344827586 train loss:  1.525978962580363 train acc: 0.4338461538461538\n",
      "val acc:  0.46551724137931033 train loss:  1.480952779452006 train acc: 0.4369230769230769\n",
      "val acc:  0.41379310344827586 train loss:  1.4564378261566162 train acc: 0.48615384615384616\n",
      "val acc:  0.3620689655172414 train loss:  1.4053426356542678 train acc: 0.46153846153846156\n",
      "val acc:  0.39655172413793105 train loss:  1.3524899255661738 train acc: 0.49846153846153846\n",
      "val acc:  0.39655172413793105 train loss:  1.3524056332451957 train acc: 0.48307692307692307\n",
      "val acc:  0.4482758620689655 train loss:  1.2518985044388544 train acc: 0.5384615384615384\n",
      "val acc:  0.5 train loss:  1.1773458123207092 train acc: 0.5753846153846154\n",
      "val acc:  0.43103448275862066 train loss:  1.119252500079927 train acc: 0.6030769230769231\n",
      "val acc:  0.5517241379310345 train loss:  1.0495710713522775 train acc: 0.6246153846153846\n",
      "val acc:  0.5862068965517241 train loss:  0.9727171205338978 train acc: 0.7107692307692308\n",
      "val acc:  0.6896551724137931 train loss:  0.8861768387612843 train acc: 0.7784615384615384\n",
      "val acc:  0.6379310344827587 train loss:  0.8025834475244794 train acc: 0.7938461538461539\n",
      "val acc:  0.6379310344827587 train loss:  0.747642077150799 train acc: 0.8061538461538461\n",
      "val acc:  0.6724137931034483 train loss:  0.7004889476866949 train acc: 0.803076923076923\n",
      "val acc:  0.5689655172413793 train loss:  0.6579961365177518 train acc: 0.8369230769230769\n",
      "val acc:  0.7241379310344828 train loss:  0.6558672728992644 train acc: 0.8\n",
      "val acc:  0.7241379310344828 train loss:  0.5612969540414356 train acc: 0.8492307692307692\n",
      "val acc:  0.6896551724137931 train loss:  0.48407974413463045 train acc: 0.8830769230769231\n",
      "val acc:  0.7586206896551724 train loss:  0.45194820704914274 train acc: 0.9138461538461539\n",
      "val acc:  0.7413793103448276 train loss:  0.4290717606033598 train acc: 0.8830769230769231\n",
      "val acc:  0.7241379310344828 train loss:  0.41178796617757707 train acc: 0.9046153846153846\n",
      "val acc:  0.8103448275862069 train loss:  0.3494232516913187 train acc: 0.9292307692307692\n",
      "val acc:  0.7758620689655172 train loss:  0.31514592113949 train acc: 0.9415384615384615\n",
      "val acc:  0.7586206896551724 train loss:  0.30556618741580416 train acc: 0.9261538461538461\n",
      "val acc:  0.7413793103448276 train loss:  0.2792548867208617 train acc: 0.9292307692307692\n",
      "val acc:  0.7586206896551724 train loss:  0.250429881470544 train acc: 0.9538461538461539\n",
      "val acc:  0.7758620689655172 train loss:  0.2154691322218804 train acc: 0.9692307692307692\n",
      "val acc:  0.7586206896551724 train loss:  0.19512966771920523 train acc: 0.9723076923076923\n",
      "val acc:  0.7241379310344828 train loss:  0.19657318116653533 train acc: 0.9784615384615385\n",
      "val acc:  0.7586206896551724 train loss:  0.18102292610066278 train acc: 0.9753846153846154\n",
      "val acc:  0.7413793103448276 train loss:  0.1451058497741109 train acc: 0.9784615384615385\n",
      "val acc:  0.7931034482758621 train loss:  0.13122306302899406 train acc: 0.9876923076923076\n",
      "val acc:  0.7586206896551724 train loss:  0.1542539791691871 train acc: 0.9815384615384616\n",
      "val acc:  0.7758620689655172 train loss:  0.13922707878407978 train acc: 0.9907692307692307\n",
      "val acc:  0.7586206896551724 train loss:  0.13101160526275635 train acc: 0.9723076923076923\n",
      "val acc:  0.7068965517241379 train loss:  0.1417273556192716 train acc: 0.9661538461538461\n",
      "val acc:  0.7586206896551724 train loss:  0.114619358841862 train acc: 0.9846153846153847\n",
      "val acc:  0.7413793103448276 train loss:  0.14849780411237762 train acc: 0.96\n",
      "val acc:  0.7241379310344828 train loss:  0.13994740410929635 train acc: 0.9815384615384616\n",
      "val acc:  0.7758620689655172 train loss:  0.1956810009266649 train acc: 0.9476923076923077\n",
      "val acc:  0.7586206896551724 train loss:  0.09900338894554547 train acc: 0.9846153846153847\n",
      "val acc:  0.7413793103448276 train loss:  0.08449460317691167 train acc: 0.9876923076923076\n",
      "val acc:  0.7586206896551724 train loss:  0.08208789357117244 train acc: 0.9846153846153847\n",
      "val acc:  0.7241379310344828 train loss:  0.08182091230437868 train acc: 0.9876923076923076\n",
      "val acc:  0.7586206896551724 train loss:  0.05916480489429973 train acc: 0.9969230769230769\n",
      "val acc:  0.7413793103448276 train loss:  0.050320634174914586 train acc: 0.9969230769230769\n",
      "val acc:  0.7241379310344828 train loss:  0.044343061035587675 train acc: 0.9969230769230769\n",
      "val acc:  0.7586206896551724 train loss:  0.042590755437101634 train acc: 0.9969230769230769\n",
      "val acc:  0.7758620689655172 train loss:  0.04739366968472799 train acc: 0.9969230769230769\n",
      "val acc:  0.7068965517241379 train loss:  0.04005574913961547 train acc: 1.0\n",
      "val acc:  0.7931034482758621 train loss:  0.06437701856096585 train acc: 0.9907692307692307\n",
      "val acc:  0.7586206896551724 train loss:  0.06709517245846135 train acc: 0.9784615384615385\n",
      "val acc:  0.7413793103448276 train loss:  0.10305841754944552 train acc: 0.9692307692307692\n",
      "val acc:  0.7758620689655172 train loss:  0.06897537969052792 train acc: 0.9907692307692307\n",
      "val acc:  0.7586206896551724 train loss:  0.029223502125768436 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.026703074397075744 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.024943344028932706 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.022329495421477726 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.021651766395994594 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.020887167946923347 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.020108774038297788 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.020356403131570135 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.018696198993850322 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.018424447581526778 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.01779976056977397 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.01736962240898893 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.01609169274923347 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.01613366226887419 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.015546137600072793 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.014631002742264951 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.014385547444579146 train acc: 1.0\n",
      "val acc:  0.7413793103448276 train loss:  0.015401478856801987 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.014389187807128542 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.013326452512826239 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.012976583581240405 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.012758490008612474 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.012393780496148836 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.012401273341051169 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.01233990645656983 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.011392886395610514 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.011323692986652964 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.01100298363183226 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.01063444735925822 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.011022292432330903 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.010054069571197033 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.009727775695778075 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.009757003214742457 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.009386720313202767 train acc: 1.0\n",
      "val acc:  0.7758620689655172 train loss:  0.00955386022992787 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.008804923145189172 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.008618366815859363 train acc: 1.0\n",
      "val acc:  0.7413793103448276 train loss:  0.008904758530358473 train acc: 1.0\n",
      "val acc:  0.7413793103448276 train loss:  0.010057222763342517 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.008381514122620934 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.00829355629338395 train acc: 1.0\n",
      "val acc:  0.7586206896551724 train loss:  0.00780786086051237 train acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "crossEntropy = CrossEntropyLoss()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # train_feature_w2v, train_label_w2v = shuffle(train_feature_w2v, train_label_w2v)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    # random.shuffle(train_pred)\n",
    "    for feature, label in train_dataloader:\n",
    "        # print(feature.shape)\n",
    "        output = model(feature.view((feature.shape[0], -1))).logits\n",
    "        loss = crossEntropy(output.view((-1, 5)), label.view((-1,)))\n",
    "        correct += torch.sum(torch.argmax(output, dim = 1) == label).item()\n",
    "        all += feature.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "    train_acc = correct / all\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    model.eval()\n",
    "    for feature, label in val_dataloader:\n",
    "        all += feature.shape[0]\n",
    "        output = model(feature.view((feature.shape[0], -1))).logits\n",
    "        # print(torch.argmax(output, dim = 1), label)\n",
    "        correct += torch.sum(torch.argmax(output, dim = 1) == label).item()\n",
    "\n",
    "    print(\"val acc: \",correct / all, \"train loss: \", np.mean(loss_list), \"train acc:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 4, 0, 4, 1, 4, 0, 0, 4], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 4, 0, 4, 1, 4, 0, 0, 4], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_flattened = []\n",
    "dataset_pred_label = []\n",
    "\n",
    "dataset_pred_flattened_val = []\n",
    "dataset_pred_label_val = []\n",
    "\n",
    "for i, j in train_pred:\n",
    "    dataset_pred_flattened.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label.append(j.item())\n",
    "\n",
    "\n",
    "for i, j in val_pred:\n",
    "    dataset_pred_flattened_val.append(i.view((-1,)).cpu().numpy())\n",
    "    dataset_pred_label_val.append(j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43103448275862066"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf_perceptron = SGDClassifier(max_iter=1000)\n",
    "clf_perceptron.fit(dataset_pred_flattened, dataset_pred_label)\n",
    "\n",
    "accuracy_score(dataset_pred_label_val, clf_perceptron.predict(dataset_pred_flattened_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(output, dim = 1) == torch.tensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 4, 0, 4, 1, 4, 0, 0, 4], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m Perceptron\n\u001b[0;32m      5\u001b[0m clf_perceptron \u001b[39m=\u001b[39m Perceptron()\n\u001b[1;32m----> 6\u001b[0m clf_perceptron\u001b[39m.\u001b[39mfit(train_feature, train_label)\n\u001b[0;32m      8\u001b[0m accuracy_score(test_label, clf_perceptron\u001b[39m.\u001b[39mpredict(test_feature))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_feature' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_perceptron = Perceptron()\n",
    "clf_perceptron.fit(train_feature, train_label)\n",
    "\n",
    "accuracy_score(test_label, clf_perceptron.predict(test_feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "566_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
