{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorchvideo.layers.utils import set_attributes\n",
    "from pytorchvideo.models.weight_init import init_net_weights\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a general Net models with a list of blocks for video recognition.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                         Input\n",
    "                                           ↓\n",
    "                                         Block 1\n",
    "                                           ↓\n",
    "                                           .\n",
    "                                           .\n",
    "                                           .\n",
    "                                           ↓\n",
    "                                         Block N\n",
    "                                           ↓\n",
    "\n",
    "    The ResNet builder can be found in `create_resnet`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, blocks: nn.ModuleList) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            blocks (torch.nn.module_list): the list of block modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert blocks is not None\n",
    "        self.blocks = blocks\n",
    "        init_net_weights(self)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for idx in range(len(self.blocks)):\n",
    "            x = self.blocks[idx](x)\n",
    "            # print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "import math\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fvcore.nn.squeeze_excitation import SqueezeExcitation\n",
    "from pytorchvideo.layers.convolutions import Conv2plus1d\n",
    "from pytorchvideo.layers.swish import Swish\n",
    "from pytorchvideo.layers.utils import round_repeats, round_width, set_attributes\n",
    "from pytorchvideo.models.head import ResNetBasicHead\n",
    "# from pytorchvideo.models.net import Net\n",
    "from pytorchvideo.models.resnet import BottleneckBlock, ResBlock, ResStage\n",
    "from pytorchvideo.models.stem import ResNetBasicStem\n",
    "\n",
    "\n",
    "def create_x3d_stem(\n",
    "    *,\n",
    "    # Conv configs.\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    conv_kernel_size: Tuple[int] = (5, 3, 3),\n",
    "    conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    conv_padding: Tuple[int] = (2, 1, 1),\n",
    "    # BN configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates the stem layer for X3D. It performs spatial Conv, temporal Conv, BN, and Relu.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                        Conv_xy\n",
    "                                           ↓\n",
    "                                        Conv_t\n",
    "                                           ↓\n",
    "                                     Normalization\n",
    "                                           ↓\n",
    "                                       Activation\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): input channel size of the convolution.\n",
    "        out_channels (int): output channel size of the convolution.\n",
    "        conv_kernel_size (tuple): convolutional kernel size(s).\n",
    "        conv_stride (tuple): convolutional stride size(s).\n",
    "        conv_padding (tuple): convolutional padding size(s).\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer, options\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer, options\n",
    "            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing\n",
    "            activation).\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): X3D stem layer.\n",
    "    \"\"\"\n",
    "    conv_xy_module = nn.Conv3d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=(1, conv_kernel_size[1], conv_kernel_size[2]),\n",
    "        stride=(1, conv_stride[1], conv_stride[2]),\n",
    "        padding=(0, conv_padding[1], conv_padding[2]),\n",
    "        bias=False,\n",
    "    )\n",
    "    conv_t_module = nn.Conv3d(\n",
    "        in_channels=out_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=(conv_kernel_size[0], 1, 1),\n",
    "        stride=(conv_stride[0], 1, 1),\n",
    "        padding=(conv_padding[0], 0, 0),\n",
    "        bias=False,\n",
    "        groups=out_channels,\n",
    "    )\n",
    "    stacked_conv_module = Conv2plus1d(\n",
    "        conv_t=conv_xy_module,\n",
    "        norm=None,\n",
    "        activation=None,\n",
    "        conv_xy=conv_t_module,\n",
    "    )\n",
    "\n",
    "    norm_module = (\n",
    "        None\n",
    "        if norm is None\n",
    "        else norm(num_features=out_channels, eps=norm_eps, momentum=norm_momentum)\n",
    "    )\n",
    "    activation_module = None if activation is None else activation()\n",
    "\n",
    "    return ResNetBasicStem(\n",
    "        conv=stacked_conv_module,\n",
    "        norm=norm_module,\n",
    "        activation=activation_module,\n",
    "        pool=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_x3d_bottleneck_block(\n",
    "    *,\n",
    "    # Convolution configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
    "    conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    # Norm configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    se_ratio: float = 0.0625,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    "    inner_act: Callable = Swish,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Bottleneck block for X3D: a sequence of Conv, Normalization with optional SE block,\n",
    "    and Activations repeated in the following order:\n",
    "\n",
    "    ::\n",
    "\n",
    "                                    Conv3d (conv_a)\n",
    "                                           ↓\n",
    "                                 Normalization (norm_a)\n",
    "                                           ↓\n",
    "                                   Activation (act_a)\n",
    "                                           ↓\n",
    "                                    Conv3d (conv_b)\n",
    "                                           ↓\n",
    "                                 Normalization (norm_b)\n",
    "                                           ↓\n",
    "                                 Squeeze-and-Excitation\n",
    "                                           ↓\n",
    "                                   Activation (act_b)\n",
    "                                           ↓\n",
    "                                    Conv3d (conv_c)\n",
    "                                           ↓\n",
    "                                 Normalization (norm_c)\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): input channel size to the bottleneck block.\n",
    "        dim_inner (int): intermediate channel size of the bottleneck.\n",
    "        dim_out (int): output channel size of the bottleneck.\n",
    "        conv_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        conv_stride (tuple): convolutional stride size(s) for conv_b.\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer, examples\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "        se_ratio (float): if > 0, apply SE to the 3x3x3 conv, with the SE\n",
    "            channel dimensionality being se_ratio times the 3x3x3 conv dim.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer, examples\n",
    "            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing\n",
    "            activation).\n",
    "        inner_act (callable): whether use Swish activation for act_b or not.\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): X3D bottleneck block.\n",
    "    \"\"\"\n",
    "    # 1x1x1 Conv\n",
    "    conv_a = nn.Conv3d(\n",
    "        in_channels=dim_in, out_channels=dim_inner, kernel_size=(1, 1, 1), bias=False\n",
    "    )\n",
    "    norm_a = (\n",
    "        None\n",
    "        if norm is None\n",
    "        else norm(num_features=dim_inner, eps=norm_eps, momentum=norm_momentum)\n",
    "    )\n",
    "    act_a = None if activation is None else activation()\n",
    "\n",
    "    # 3x3x3 Conv\n",
    "    conv_b = nn.Conv3d(\n",
    "        in_channels=dim_inner,\n",
    "        out_channels=dim_inner,\n",
    "        kernel_size=conv_kernel_size,\n",
    "        stride=conv_stride,\n",
    "        padding=[size // 2 for size in conv_kernel_size],\n",
    "        bias=False,\n",
    "        groups=dim_inner,\n",
    "        dilation=(1, 1, 1),\n",
    "    )\n",
    "    se = (\n",
    "        SqueezeExcitation(\n",
    "            num_channels=dim_inner,\n",
    "            num_channels_reduced=round_width(dim_inner, se_ratio),\n",
    "            is_3d=True,\n",
    "        )\n",
    "        if se_ratio > 0.0\n",
    "        else nn.Identity()\n",
    "    )\n",
    "    norm_b = nn.Sequential(\n",
    "        (\n",
    "            nn.Identity()\n",
    "            if norm is None\n",
    "            else norm(num_features=dim_inner, eps=norm_eps, momentum=norm_momentum)\n",
    "        ),\n",
    "        se,\n",
    "    )\n",
    "    act_b = None if inner_act is None else inner_act()\n",
    "\n",
    "    # 1x1x1 Conv\n",
    "    conv_c = nn.Conv3d(\n",
    "        in_channels=dim_inner, out_channels=dim_out, kernel_size=(1, 1, 1), bias=False\n",
    "    )\n",
    "    norm_c = (\n",
    "        None\n",
    "        if norm is None\n",
    "        else norm(num_features=dim_out, eps=norm_eps, momentum=norm_momentum)\n",
    "    )\n",
    "\n",
    "    return BottleneckBlock(\n",
    "        conv_a=conv_a,\n",
    "        norm_a=norm_a,\n",
    "        act_a=act_a,\n",
    "        conv_b=conv_b,\n",
    "        norm_b=norm_b,\n",
    "        act_b=act_b,\n",
    "        conv_c=conv_c,\n",
    "        norm_c=norm_c,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_x3d_res_block(\n",
    "    *,\n",
    "    # Bottleneck Block configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
    "    use_shortcut: bool = True,\n",
    "    # Conv configs.\n",
    "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
    "    conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    # Norm configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    se_ratio: float = 0.0625,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    "    inner_act: Callable = Swish,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Residual block for X3D. Performs a summation between an identity shortcut in branch1 and a\n",
    "    main block in branch2. When the input and output dimensions are different, a\n",
    "    convolution followed by a normalization will be performed.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                         Input\n",
    "                                           |-------+\n",
    "                                           ↓       |\n",
    "                                         Block     |\n",
    "                                           ↓       |\n",
    "                                       Summation ←-+\n",
    "                                           ↓\n",
    "                                       Activation\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): input channel size to the bottleneck block.\n",
    "        dim_inner (int): intermediate channel size of the bottleneck.\n",
    "        dim_out (int): output channel size of the bottleneck.\n",
    "        bottleneck (callable): a callable for create_x3d_bottleneck_block.\n",
    "\n",
    "        conv_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        conv_stride (tuple): convolutional stride size(s) for conv_b.\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer, examples\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "        se_ratio (float): if > 0, apply SE to the 3x3x3 conv, with the SE\n",
    "            channel dimensionality being se_ratio times the 3x3x3 conv dim.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer, examples\n",
    "            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing\n",
    "            activation).\n",
    "        inner_act (callable): whether use Swish activation for act_b or not.\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): X3D block layer.\n",
    "    \"\"\"\n",
    "\n",
    "    norm_model = None\n",
    "    if norm is not None and dim_in != dim_out:\n",
    "        norm_model = norm(num_features=dim_out)\n",
    "\n",
    "    return ResBlock(\n",
    "        branch1_conv=nn.Conv3d(\n",
    "            dim_in,\n",
    "            dim_out,\n",
    "            kernel_size=(1, 1, 1),\n",
    "            stride=conv_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        if (dim_in != dim_out or np.prod(conv_stride) > 1) and use_shortcut\n",
    "        else None,\n",
    "        branch1_norm=norm_model if dim_in != dim_out and use_shortcut else None,\n",
    "        branch2=bottleneck(\n",
    "            dim_in=dim_in,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            conv_stride=conv_stride,\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            se_ratio=se_ratio,\n",
    "            activation=activation,\n",
    "            inner_act=inner_act,\n",
    "        ),\n",
    "        activation=None if activation is None else activation(),\n",
    "        branch_fusion=lambda x, y: x + y,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_x3d_res_stage(\n",
    "    *,\n",
    "    # Stage configs.\n",
    "    depth: int,\n",
    "    # Bottleneck Block configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
    "    # Conv configs.\n",
    "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
    "    conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    # Norm configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    se_ratio: float = 0.0625,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    "    inner_act: Callable = Swish,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create Residual Stage, which composes sequential blocks that make up X3D.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                        Input\n",
    "                                           ↓\n",
    "                                       ResBlock\n",
    "                                           ↓\n",
    "                                           .\n",
    "                                           .\n",
    "                                           .\n",
    "                                           ↓\n",
    "                                       ResBlock\n",
    "\n",
    "    Args:\n",
    "\n",
    "        depth (init): number of blocks to create.\n",
    "\n",
    "        dim_in (int): input channel size to the bottleneck block.\n",
    "        dim_inner (int): intermediate channel size of the bottleneck.\n",
    "        dim_out (int): output channel size of the bottleneck.\n",
    "        bottleneck (callable): a callable for create_x3d_bottleneck_block.\n",
    "\n",
    "        conv_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        conv_stride (tuple): convolutional stride size(s) for conv_b.\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer, examples\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "        se_ratio (float): if > 0, apply SE to the 3x3x3 conv, with the SE\n",
    "            channel dimensionality being se_ratio times the 3x3x3 conv dim.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer, examples\n",
    "            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing\n",
    "            activation).\n",
    "        inner_act (callable): whether use Swish activation for act_b or not.\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): X3D stage layer.\n",
    "    \"\"\"\n",
    "    res_blocks = []\n",
    "    for idx in range(depth):\n",
    "        block = create_x3d_res_block(\n",
    "            dim_in=dim_in if idx == 0 else dim_out,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            bottleneck=bottleneck,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            conv_stride=conv_stride if idx == 0 else (1, 1, 1),\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            se_ratio=(se_ratio if (idx + 1) % 2 else 0.0),\n",
    "            activation=activation,\n",
    "            inner_act=inner_act,\n",
    "        )\n",
    "        res_blocks.append(block)\n",
    "\n",
    "    return ResStage(res_blocks=nn.ModuleList(res_blocks))\n",
    "\n",
    "\n",
    "def create_x3d_head(\n",
    "    *,\n",
    "    # Projection configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    num_classes: int,\n",
    "    # Pooling configs.\n",
    "    pool_act: Callable = nn.ReLU,\n",
    "    pool_kernel_size: Tuple[int] = (13, 5, 5),\n",
    "    # BN configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    bn_lin5_on=False,\n",
    "    # Dropout configs.\n",
    "    dropout_rate: float = 0.5,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.Softmax,\n",
    "    # Output configs.\n",
    "    output_with_global_average: bool = True,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates X3D head. This layer performs an projected pooling operation followed\n",
    "    by an dropout, a fully-connected projection, an activation layer and a global\n",
    "    spatiotemporal averaging.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                     ProjectedPool\n",
    "                                           ↓\n",
    "                                        Dropout\n",
    "                                           ↓\n",
    "                                       Projection\n",
    "                                           ↓\n",
    "                                       Activation\n",
    "                                           ↓\n",
    "                                       Averaging\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): input channel size of the X3D head.\n",
    "        dim_inner (int): intermediate channel size of the X3D head.\n",
    "        dim_out (int): output channel size of the X3D head.\n",
    "        num_classes (int): the number of classes for the video dataset.\n",
    "\n",
    "        pool_act (callable): a callable that constructs resnet pool activation\n",
    "            layer such as nn.ReLU.\n",
    "        pool_kernel_size (tuple): pooling kernel size(s) when not using adaptive\n",
    "            pooling.\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer, examples\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "        bn_lin5_on (bool): if True, perform normalization on the features\n",
    "            before the classifier.\n",
    "\n",
    "        dropout_rate (float): dropout rate.\n",
    "\n",
    "        activation (callable): a callable that constructs resnet head activation\n",
    "            layer, examples include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not\n",
    "            applying activation).\n",
    "\n",
    "        output_with_global_average (bool): if True, perform global averaging on temporal\n",
    "            and spatial dimensions and reshape output to batch_size x out_features.\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): X3D head layer.\n",
    "    \"\"\"\n",
    "    # print(locals())\n",
    "    pre_conv_module = nn.Conv3d(\n",
    "        in_channels=dim_in, out_channels=dim_inner, kernel_size=(1, 1, 1), bias=False\n",
    "    )\n",
    "\n",
    "    pre_norm_module = norm(num_features=dim_inner, eps=norm_eps, momentum=norm_momentum)\n",
    "    pre_act_module = None if pool_act is None else pool_act()\n",
    "\n",
    "    if pool_kernel_size is None:\n",
    "        pool_module = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "    else:\n",
    "        pool_module = nn.AvgPool3d(pool_kernel_size, stride=1)\n",
    "\n",
    "    post_conv_module = nn.Conv3d(\n",
    "        in_channels=dim_inner, out_channels=dim_out, kernel_size=(1, 1, 1), bias=False\n",
    "    )\n",
    "\n",
    "    if bn_lin5_on:\n",
    "        post_norm_module = norm(\n",
    "            num_features=dim_out, eps=norm_eps, momentum=norm_momentum\n",
    "        )\n",
    "    else:\n",
    "        post_norm_module = None\n",
    "    post_act_module = None if pool_act is None else pool_act()\n",
    "\n",
    "    projected_pool_module = ProjectedPool(\n",
    "        pre_conv=pre_conv_module,\n",
    "        pre_norm=pre_norm_module,\n",
    "        pre_act=pre_act_module,\n",
    "        pool=pool_module,\n",
    "        post_conv=post_conv_module,\n",
    "        post_norm=post_norm_module,\n",
    "        post_act=post_act_module,\n",
    "    )\n",
    "\n",
    "    if activation is None:\n",
    "        activation_module = None\n",
    "    elif activation == nn.Softmax:\n",
    "        activation_module = activation(dim=1)\n",
    "    elif activation == nn.Sigmoid:\n",
    "        activation_module = activation()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"{} is not supported as an activation\" \"function.\".format(activation)\n",
    "        )\n",
    "\n",
    "    if output_with_global_average:\n",
    "        output_pool = nn.AdaptiveAvgPool3d(1)\n",
    "    else:\n",
    "        output_pool = None\n",
    "\n",
    "    return ResNetBasicHead(\n",
    "        proj=nn.Linear(dim_out, num_classes, bias=True),\n",
    "        activation=activation_module,\n",
    "        pool=projected_pool_module,\n",
    "        dropout=nn.Dropout(dropout_rate) if dropout_rate > 0 else None,\n",
    "        output_pool=output_pool,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_x3d(\n",
    "    *,\n",
    "    # Input clip configs.\n",
    "    input_channel: int = 3,\n",
    "    input_clip_length: int = 13,\n",
    "    input_crop_size: int = 160,\n",
    "    # Model configs.\n",
    "    model_num_class: int = 400,\n",
    "    dropout_rate: float = 0.5,\n",
    "    width_factor: float = 2.0,\n",
    "    depth_factor: float = 2.2,\n",
    "    # Normalization configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    "    # Stem configs.\n",
    "    stem_dim_in: int = 12,\n",
    "    stem_conv_kernel_size: Tuple[int] = (5, 3, 3),\n",
    "    stem_conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    # Stage configs.\n",
    "    stage_conv_kernel_size: Tuple[Tuple[int]] = (\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "    ),\n",
    "    stage_spatial_stride: Tuple[int] = (2, 2, 2, 2),\n",
    "    stage_temporal_stride: Tuple[int] = (1, 1, 1, 1),\n",
    "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
    "    bottleneck_factor: float = 2.25,\n",
    "    se_ratio: float = 0.0625,\n",
    "    inner_act: Callable = Swish,\n",
    "    # Head configs.\n",
    "    head_dim_out: int = 2048,\n",
    "    head_pool_act: Callable = nn.ReLU,\n",
    "    head_bn_lin5_on: bool = False,\n",
    "    head_activation: Callable = nn.Softmax,\n",
    "    head_output_with_global_average: bool = True,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    X3D model builder. It builds a X3D network backbone, which is a ResNet.\n",
    "\n",
    "    Christoph Feichtenhofer.\n",
    "    \"X3D: Expanding Architectures for Efficient Video Recognition.\"\n",
    "    https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    ::\n",
    "\n",
    "                                         Input\n",
    "                                           ↓\n",
    "                                         Stem\n",
    "                                           ↓\n",
    "                                         Stage 1\n",
    "                                           ↓\n",
    "                                           .\n",
    "                                           .\n",
    "                                           .\n",
    "                                           ↓\n",
    "                                         Stage N\n",
    "                                           ↓\n",
    "                                         Head\n",
    "\n",
    "    Args:\n",
    "        input_channel (int): number of channels for the input video clip.\n",
    "        input_clip_length (int): length of the input video clip. Value for\n",
    "            different models: X3D-XS: 4; X3D-S: 13; X3D-M: 16; X3D-L: 16.\n",
    "        input_crop_size (int): spatial resolution of the input video clip.\n",
    "            Value for different models: X3D-XS: 160; X3D-S: 160; X3D-M: 224;\n",
    "            X3D-L: 312.\n",
    "\n",
    "        model_num_class (int): the number of classes for the video dataset.\n",
    "        dropout_rate (float): dropout rate.\n",
    "        width_factor (float): width expansion factor.\n",
    "        depth_factor (float): depth expansion factor. Value for different\n",
    "            models: X3D-XS: 2.2; X3D-S: 2.2; X3D-M: 2.2; X3D-L: 5.0.\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer.\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer.\n",
    "\n",
    "        stem_dim_in (int): input channel size for stem before expansion.\n",
    "        stem_conv_kernel_size (tuple): convolutional kernel size(s) of stem.\n",
    "        stem_conv_stride (tuple): convolutional stride size(s) of stem.\n",
    "\n",
    "        stage_conv_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        stage_spatial_stride (tuple): the spatial stride for each stage.\n",
    "        stage_temporal_stride (tuple): the temporal stride for each stage.\n",
    "        bottleneck_factor (float): bottleneck expansion factor for the 3x3x3 conv.\n",
    "        se_ratio (float): if > 0, apply SE to the 3x3x3 conv, with the SE\n",
    "            channel dimensionality being se_ratio times the 3x3x3 conv dim.\n",
    "        inner_act (callable): whether use Swish activation for act_b or not.\n",
    "\n",
    "        head_dim_out (int): output channel size of the X3D head.\n",
    "        head_pool_act (callable): a callable that constructs resnet pool activation\n",
    "            layer such as nn.ReLU.\n",
    "        head_bn_lin5_on (bool): if True, perform normalization on the features\n",
    "            before the classifier.\n",
    "        head_activation (callable): a callable that constructs activation layer.\n",
    "        head_output_with_global_average (bool): if True, perform global averaging on\n",
    "            the head output.\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): the X3D network.\n",
    "    \"\"\"\n",
    "\n",
    "    torch._C._log_api_usage_once(\"PYTORCHVIDEO.model.create_x3d\")\n",
    "\n",
    "    blocks = []\n",
    "    # Create stem for X3D.\n",
    "    stem_dim_out = round_width(stem_dim_in, width_factor)\n",
    "    stem = create_x3d_stem(\n",
    "        in_channels=input_channel,\n",
    "        out_channels=stem_dim_out,\n",
    "        conv_kernel_size=stem_conv_kernel_size,\n",
    "        conv_stride=stem_conv_stride,\n",
    "        conv_padding=[size // 2 for size in stem_conv_kernel_size],\n",
    "        norm=norm,\n",
    "        norm_eps=norm_eps,\n",
    "        norm_momentum=norm_momentum,\n",
    "        activation=activation,\n",
    "    )\n",
    "    blocks.append(stem)\n",
    "\n",
    "    # Compute the depth and dimension for each stage\n",
    "    stage_depths = [1, 2, 5, 3]\n",
    "    exp_stage = 2.0\n",
    "    stage_dim1 = stem_dim_in\n",
    "    stage_dim2 = round_width(stage_dim1, exp_stage, divisor=8)\n",
    "    stage_dim3 = round_width(stage_dim2, exp_stage, divisor=8)\n",
    "    stage_dim4 = round_width(stage_dim3, exp_stage, divisor=8)\n",
    "    stage_dims = [stage_dim1, stage_dim2, stage_dim3, stage_dim4]\n",
    "\n",
    "    dim_in = stem_dim_out\n",
    "    # Create each stage for X3D.\n",
    "    for idx in range(len(stage_depths)):\n",
    "        dim_out = round_width(stage_dims[idx], width_factor)\n",
    "        dim_inner = int(bottleneck_factor * dim_out)\n",
    "        depth = round_repeats(stage_depths[idx], depth_factor)\n",
    "\n",
    "        stage_conv_stride = (\n",
    "            stage_temporal_stride[idx],\n",
    "            stage_spatial_stride[idx],\n",
    "            stage_spatial_stride[idx],\n",
    "        )\n",
    "\n",
    "        stage = create_x3d_res_stage(\n",
    "            depth=depth,\n",
    "            dim_in=dim_in,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            bottleneck=bottleneck,\n",
    "            conv_kernel_size=stage_conv_kernel_size[idx],\n",
    "            conv_stride=stage_conv_stride,\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            se_ratio=se_ratio,\n",
    "            activation=activation,\n",
    "            inner_act=inner_act,\n",
    "        )\n",
    "        blocks.append(stage)\n",
    "        dim_in = dim_out\n",
    "\n",
    "    # Create head for X3D.\n",
    "    total_spatial_stride = stem_conv_stride[1] * np.prod(stage_spatial_stride)\n",
    "    total_temporal_stride = stem_conv_stride[0] * np.prod(stage_temporal_stride)\n",
    "\n",
    "    assert (\n",
    "        input_clip_length >= total_temporal_stride\n",
    "    ), \"Clip length doesn't match temporal stride!\"\n",
    "    assert (\n",
    "        input_crop_size >= total_spatial_stride\n",
    "    ), \"Crop size doesn't match spatial stride!\"\n",
    "\n",
    "    head_pool_kernel_size = (\n",
    "        input_clip_length // total_temporal_stride,\n",
    "        int(math.ceil(input_crop_size / total_spatial_stride)),\n",
    "        int(math.ceil(input_crop_size / total_spatial_stride)),\n",
    "    )\n",
    "\n",
    "    head = create_x3d_head(\n",
    "        dim_in=dim_out,\n",
    "        dim_inner=dim_inner,\n",
    "        dim_out=head_dim_out,\n",
    "        num_classes=model_num_class,\n",
    "        pool_act=head_pool_act,\n",
    "        pool_kernel_size=head_pool_kernel_size,\n",
    "        norm=norm,\n",
    "        norm_eps=norm_eps,\n",
    "        norm_momentum=norm_momentum,\n",
    "        bn_lin5_on=head_bn_lin5_on,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=head_activation,\n",
    "        output_with_global_average=head_output_with_global_average,\n",
    "    )\n",
    "    blocks.append(head)\n",
    "    return Net(blocks=nn.ModuleList(blocks))\n",
    "\n",
    "\n",
    "class ProjectedPool(nn.Module):\n",
    "    \"\"\"\n",
    "    A pooling module augmented with Conv, Normalization and Activation both\n",
    "    before and after pooling for the head layer of X3D.\n",
    "\n",
    "    ::\n",
    "\n",
    "                                    Conv3d (pre_conv)\n",
    "                                           ↓\n",
    "                                 Normalization (pre_norm)\n",
    "                                           ↓\n",
    "                                   Activation (pre_act)\n",
    "                                           ↓\n",
    "                                        Pool3d\n",
    "                                           ↓\n",
    "                                    Conv3d (post_conv)\n",
    "                                           ↓\n",
    "                                 Normalization (post_norm)\n",
    "                                           ↓\n",
    "                                   Activation (post_act)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        pre_conv: nn.Module = None,\n",
    "        pre_norm: nn.Module = None,\n",
    "        pre_act: nn.Module = None,\n",
    "        pool: nn.Module = None,\n",
    "        post_conv: nn.Module = None,\n",
    "        post_norm: nn.Module = None,\n",
    "        post_act: nn.Module = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pre_conv (torch.nn.modules): convolutional module.\n",
    "            pre_norm (torch.nn.modules): normalization module.\n",
    "            pre_act (torch.nn.modules): activation module.\n",
    "            pool (torch.nn.modules): pooling module.\n",
    "            post_conv (torch.nn.modules): convolutional module.\n",
    "            post_norm (torch.nn.modules): normalization module.\n",
    "            post_act (torch.nn.modules): activation module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        set_attributes(self, locals())\n",
    "        assert self.pre_conv is not None\n",
    "        assert self.pool is not None\n",
    "        assert self.post_conv is not None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pre_conv(x)\n",
    "\n",
    "        if self.pre_norm is not None:\n",
    "            x = self.pre_norm(x)\n",
    "        if self.pre_act is not None:\n",
    "            x = self.pre_act(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = self.post_conv(x)\n",
    "\n",
    "        if self.post_norm is not None:\n",
    "            x = self.post_norm(x)\n",
    "        if self.post_act is not None:\n",
    "            x = self.post_act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "# from pytorchvideo.models.x3d import create_x3d\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "\n",
    "root_dir = \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics\"\n",
    "checkpoint_paths = {\n",
    "    \"x3d_xs\": f\"{root_dir}/X3D_XS.pyth\",\n",
    "    \"x3d_s\": f\"{root_dir}/X3D_S.pyth\",\n",
    "    \"x3d_m\": f\"{root_dir}/X3D_M.pyth\",\n",
    "    \"x3d_l\": f\"{root_dir}/X3D_L.pyth\",\n",
    "}\n",
    "\n",
    "\n",
    "def _x3d(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "    **kwargs: Any,\n",
    ") -> nn.Module:\n",
    "    model = create_x3d(**kwargs)\n",
    "    if pretrained and checkpoint_path is not None:\n",
    "        # All models are loaded onto CPU by default\n",
    "        checkpoint = load_state_dict_from_url(\n",
    "            checkpoint_path, progress=progress, map_location=\"cpu\"\n",
    "        )\n",
    "        state_dict = checkpoint[\"model_state\"]\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def x3d_xs(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    r\"\"\"\n",
    "    X3D-XS model architecture [1] trained on the Kinetics dataset.\n",
    "    Model with pretrained weights has top1 accuracy of 69.12.\n",
    "\n",
    "    [1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
    "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on the Kinetics dataset\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        kwargs: use these to modify any of the other model settings. All the\n",
    "            options are defined in pytorchvideo/models/x3d.py\n",
    "\n",
    "    NOTE: to use the pretrained model, do not modify the model configuration\n",
    "    via the kwargs. Only modify settings via kwargs to initialize a new model\n",
    "    without pretrained weights.\n",
    "    \"\"\"\n",
    "    return _x3d(\n",
    "        pretrained=pretrained,\n",
    "        progress=progress,\n",
    "        checkpoint_path=checkpoint_paths[\"x3d_xs\"],\n",
    "        input_clip_length=4,\n",
    "        input_crop_size=160,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def x3d_s(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    X3D-XS model architecture [1] trained on the Kinetics dataset.\n",
    "    Model with pretrained weights has top1 accuracy of 73.33.\n",
    "\n",
    "    [1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
    "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on the Kinetics dataset\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        kwargs: use these to modify any of the other model settings. All the\n",
    "            options are defined in pytorchvideo/models/x3d.py\n",
    "\n",
    "    NOTE: to use the pretrained model, do not modify the model configuration\n",
    "    via the kwargs. Only modify settings via kwargs to initialize a new model\n",
    "    without pretrained weights.\n",
    "    \"\"\"\n",
    "    return _x3d(\n",
    "        pretrained=pretrained,\n",
    "        progress=progress,\n",
    "        checkpoint_path=checkpoint_paths[\"x3d_s\"],\n",
    "        input_clip_length=13,\n",
    "        input_crop_size=160,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def x3d_m(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    X3D-XS model architecture [1] trained on the Kinetics dataset.\n",
    "    Model with pretrained weights has top1 accuracy of 75.94.\n",
    "\n",
    "    [1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
    "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on the Kinetics dataset\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        kwargs: use these to modify any of the other model settings. All the\n",
    "            options are defined in pytorchvideo/models/x3d.py\n",
    "\n",
    "    NOTE: to use the pretrained model, do not modify the model configuration\n",
    "    via the kwargs. Only modify settings via kwargs to initialize a new model\n",
    "    without pretrained weights.\n",
    "    \"\"\"\n",
    "    return _x3d(\n",
    "        pretrained=pretrained,\n",
    "        progress=progress,\n",
    "        checkpoint_path=checkpoint_paths[\"x3d_m\"],\n",
    "        input_clip_length=16,\n",
    "        input_crop_size=224,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def x3d_l(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    X3D-XS model architecture [1] trained on the Kinetics dataset.\n",
    "    Model with pretrained weights has top1 accuracy of 77.44.\n",
    "\n",
    "    [1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
    "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on the Kinetics dataset\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        kwargs: use these to modify any of the other model settings. All the\n",
    "            options are defined in pytorchvideo/models/x3d.py\n",
    "\n",
    "    NOTE: to use the pretrained model, do not modify the model configuration\n",
    "    via the kwargs. Only modify settings via kwargs to initialize a new model\n",
    "    without pretrained weights.\n",
    "    \"\"\"\n",
    "    return _x3d(\n",
    "        pretrained=pretrained,\n",
    "        progress=progress,\n",
    "        checkpoint_path=checkpoint_paths[\"x3d_l\"],\n",
    "        input_clip_length=16,\n",
    "        input_crop_size=312,\n",
    "        depth_factor=5.0,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x3d_headless(\n",
    "    *,\n",
    "    # Input clip configs.\n",
    "    input_channel: int = 3,\n",
    "    input_clip_length: int = 13,\n",
    "    input_crop_size: int = 160,\n",
    "    # Model configs.\n",
    "    model_num_class: int = 400,\n",
    "    dropout_rate: float = 0.5,\n",
    "    width_factor: float = 2.0,\n",
    "    depth_factor: float = 2.2,\n",
    "    # Normalization configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    "    # Stem configs.\n",
    "    stem_dim_in: int = 12,\n",
    "    stem_conv_kernel_size: Tuple[int] = (5, 3, 3),\n",
    "    stem_conv_stride: Tuple[int] = (1, 2, 2),\n",
    "    # Stage configs.\n",
    "    stage_conv_kernel_size: Tuple[Tuple[int]] = (\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "        (3, 3, 3),\n",
    "    ),\n",
    "    stage_spatial_stride: Tuple[int] = (2, 2, 2, 2),\n",
    "    stage_temporal_stride: Tuple[int] = (1, 1, 1, 1),\n",
    "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
    "    bottleneck_factor: float = 2.25,\n",
    "    se_ratio: float = 0.0625,\n",
    "    inner_act: Callable = Swish,\n",
    "    # Head configs.\n",
    "    head_dim_out: int = 2048,\n",
    "    head_pool_act: Callable = nn.ReLU,\n",
    "    head_bn_lin5_on: bool = False,\n",
    "    head_activation: Callable = nn.Softmax,\n",
    "    head_output_with_global_average: bool = True,\n",
    ") -> nn.Module:\n",
    "\n",
    "    torch._C._log_api_usage_once(\"PYTORCHVIDEO.model.create_x3d\")\n",
    "\n",
    "    blocks = []\n",
    "    # Create stem for X3D.\n",
    "    stem_dim_out = round_width(stem_dim_in, width_factor)\n",
    "    stem = create_x3d_stem(\n",
    "        in_channels=input_channel,\n",
    "        out_channels=stem_dim_out,\n",
    "        conv_kernel_size=stem_conv_kernel_size,\n",
    "        conv_stride=stem_conv_stride,\n",
    "        conv_padding=[size // 2 for size in stem_conv_kernel_size],\n",
    "        norm=norm,\n",
    "        norm_eps=norm_eps,\n",
    "        norm_momentum=norm_momentum,\n",
    "        activation=activation,\n",
    "    )\n",
    "    blocks.append(stem)\n",
    "\n",
    "    # Compute the depth and dimension for each stage\n",
    "    stage_depths = [1, 2, 5, 3]\n",
    "    exp_stage = 2.0\n",
    "    stage_dim1 = stem_dim_in\n",
    "    stage_dim2 = round_width(stage_dim1, exp_stage, divisor=8)\n",
    "    stage_dim3 = round_width(stage_dim2, exp_stage, divisor=8)\n",
    "    stage_dim4 = round_width(stage_dim3, exp_stage, divisor=8)\n",
    "    stage_dims = [stage_dim1, stage_dim2, stage_dim3, stage_dim4]\n",
    "\n",
    "    dim_in = stem_dim_out\n",
    "    # Create each stage for X3D.\n",
    "    for idx in range(len(stage_depths)):\n",
    "        dim_out = round_width(stage_dims[idx], width_factor)\n",
    "        dim_inner = int(bottleneck_factor * dim_out)\n",
    "        depth = round_repeats(stage_depths[idx], depth_factor)\n",
    "\n",
    "        stage_conv_stride = (\n",
    "            stage_temporal_stride[idx],\n",
    "            stage_spatial_stride[idx],\n",
    "            stage_spatial_stride[idx],\n",
    "        )\n",
    "\n",
    "        stage = create_x3d_res_stage(\n",
    "            depth=depth,\n",
    "            dim_in=dim_in,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            bottleneck=bottleneck,\n",
    "            conv_kernel_size=stage_conv_kernel_size[idx],\n",
    "            conv_stride=stage_conv_stride,\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            se_ratio=se_ratio,\n",
    "            activation=activation,\n",
    "            inner_act=inner_act,\n",
    "        )\n",
    "        blocks.append(stage)\n",
    "        dim_in = dim_out\n",
    "\n",
    "    # Create head for X3D.\n",
    "    total_spatial_stride = stem_conv_stride[1] * np.prod(stage_spatial_stride)\n",
    "    total_temporal_stride = stem_conv_stride[0] * np.prod(stage_temporal_stride)\n",
    "\n",
    "    assert (\n",
    "        input_clip_length >= total_temporal_stride\n",
    "    ), \"Clip length doesn't match temporal stride!\"\n",
    "    assert (\n",
    "        input_crop_size >= total_spatial_stride\n",
    "    ), \"Crop size doesn't match spatial stride!\"\n",
    "\n",
    "    head_pool_kernel_size = (\n",
    "        input_clip_length // total_temporal_stride,\n",
    "        int(math.ceil(input_crop_size / total_spatial_stride)),\n",
    "        int(math.ceil(input_crop_size / total_spatial_stride)),\n",
    "    )\n",
    "\n",
    "    head = create_x3d_head(\n",
    "        dim_in=dim_out,\n",
    "        dim_inner=dim_inner,\n",
    "        dim_out=head_dim_out,\n",
    "        num_classes=model_num_class,\n",
    "        pool_act=head_pool_act,\n",
    "        pool_kernel_size=head_pool_kernel_size,\n",
    "        norm=norm,\n",
    "        norm_eps=norm_eps,\n",
    "        norm_momentum=norm_momentum,\n",
    "        bn_lin5_on=head_bn_lin5_on,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=head_activation,\n",
    "        output_with_global_average=head_output_with_global_average,\n",
    "    )\n",
    "    # blocks.append(head)\n",
    "    return Net(blocks=nn.ModuleList(blocks)), head\n",
    "\n",
    "checkpoint = None\n",
    "def _x3d_headless(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "    **kwargs: Any,\n",
    ") -> nn.Module:\n",
    "    global checkpoint\n",
    "    model, head = create_x3d_headless(**kwargs)\n",
    "    if pretrained and checkpoint_path is not None:\n",
    "        # All models are loaded onto CPU by default\n",
    "        checkpoint = load_state_dict_from_url(\n",
    "            checkpoint_path, progress=progress, map_location=\"cpu\"\n",
    "        )\n",
    "        state_dict = checkpoint[\"model_state\"]\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        for k in head.state_dict().keys():\n",
    "            if head.state_dict()[k].shape and head.state_dict()[k].shape == state_dict['blocks.5.' + k].shape:\n",
    "                head.state_dict()[k][:] = state_dict['blocks.5.' + k]\n",
    "    return model, head\n",
    "\n",
    "\n",
    "def x3d_s_headless(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    X3D-XS model architecture [1] trained on the Kinetics dataset.\n",
    "    Model with pretrained weights has top1 accuracy of 73.33.\n",
    "\n",
    "    [1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
    "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on the Kinetics dataset\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        kwargs: use these to modify any of the other model settings. All the\n",
    "            options are defined in pytorchvideo/models/x3d.py\n",
    "\n",
    "    NOTE: to use the pretrained model, do not modify the model configuration\n",
    "    via the kwargs. Only modify settings via kwargs to initialize a new model\n",
    "    without pretrained weights.\n",
    "    \"\"\"\n",
    "    return _x3d_headless(\n",
    "        pretrained=pretrained,\n",
    "        progress=progress,\n",
    "        checkpoint_path=checkpoint_paths[\"x3d_s\"],\n",
    "        input_clip_length=13,\n",
    "        input_crop_size=160,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# nn.ReLU()(torch.ones(size = (1, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x3d_with_regression(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained = True, hidden_dim = 4800, out_features = 4, in_between = 128, **kwargs,) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model, self.head = x3d_s_headless(pretrained = pretrained, **kwargs,)\n",
    "        self.keypoint_head_1 = nn.Linear(in_features=hidden_dim, out_features=in_between)\n",
    "        self.keypoint_head_2 = nn.Linear(in_features=in_between, out_features = out_features)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        # print(output.shape)\n",
    "        output_reshape = output.permute((0, 2, 1, 3, 4)).contiguous().view((-1, self.hidden_dim))\n",
    "        return self.head(output), self.keypoint_head_2(nn.ReLU()(self.keypoint_head_1(output_reshape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target, mask, reduction=\"mean\"):\n",
    "    out = (input - target.view((-1, 4)))**2\n",
    "    # print(out.view((-1, 4)).shape)\n",
    "    # print(mask.view((-1,)).shape)\n",
    "    out = out.view((-1, 4)) * mask.view((-1, 1))\n",
    "    if reduction == \"mean\":\n",
    "        return out[out != 0].mean()\n",
    "    elif reduction == \"None\":\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://api.labelstud.io/api/projects/{id}/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Token e4342ac4fcf98c2e1910b122cb4103c059f8bbfc',\n",
    "}\n",
    "\n",
    "response = requests.get('https://bilishorturl.ml/api/projects/3/export?exportType=JSON', headers=headers)\n",
    "\n",
    "import json\n",
    "annotations = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "keypoints_mapping = {}\n",
    "\n",
    "def getCenter(keypoints):\n",
    "    for point in keypoints:\n",
    "        point['center_x'] = point['x'] + point['width'] / 2 \n",
    "        point['center_y'] = point['y'] + point['height'] / 2\n",
    "\n",
    "# return_interpolation: When true append whether interpolated at the end\n",
    "# 1 means exist, 0 means missing\n",
    "def interpolation(keypoints, frames, return_interpolation):\n",
    "    prev = keypoints[0]['frame'] - 1\n",
    "    prev_x = 0\n",
    "    prev_y = 0\n",
    "    res = np.zeros((frames, 3 if return_interpolation else 2))\n",
    "    for i in keypoints:\n",
    "        diff = i['frame'] - prev\n",
    "        cur_x = i['center_x']\n",
    "        cur_y = i['center_y']\n",
    "        cur = i['frame']\n",
    "        for j in range(prev + 1, i['frame']):\n",
    "            # tmp = {'frame': j}\n",
    "            tmp_x = (prev_x * (cur - j) + cur_x * (j - prev)) / diff\n",
    "            tmp_y = (prev_y * (cur - j) + cur_y * (j - prev)) / diff\n",
    "\n",
    "            res[j - 1, :2] = (tmp_x / 100, tmp_y / 100)\n",
    "            if return_interpolation:\n",
    "                res[j - 1, -1] = 1\n",
    "            # tmp['interpolated'] = True\n",
    "            # res.append(tmp)\n",
    "        res[cur - 1, :2] = (cur_x / 100, cur_y / 100)\n",
    "        if return_interpolation:\n",
    "            res[cur - 1, -1] = 1\n",
    "        prev_x = cur_x\n",
    "        prev_y = cur_y\n",
    "        prev = i['frame']\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "labels_name = ['wand tip', 'wand end']\n",
    "for annotation in annotations:\n",
    "    vid_name = annotation['file_upload']\n",
    "\n",
    "    boxes = annotation['annotations'][0]['result']\n",
    "    \n",
    "    wand_end_keypoint = None\n",
    "    wand_tip_keypoint = None\n",
    "    wand_end_framesCount = None\n",
    "    wand_tip_framesCount = None\n",
    "\n",
    "    for i in boxes:\n",
    "        if 'labels' not in i['value'].keys():\n",
    "            continue\n",
    "        if i['value']['labels'][0] == labels_name[0]:\n",
    "            wand_tip_keypoint = i['value']['sequence']\n",
    "            wand_tip_framesCount = i['value']['framesCount']\n",
    "        elif i['value']['labels'][0] == labels_name[1]:\n",
    "            wand_end_keypoint = i['value']['sequence']\n",
    "            wand_end_framesCount = i['value']['framesCount']\n",
    "    \n",
    "    assert wand_tip_keypoint and wand_end_keypoint, f\"missing annotations for {annotation['id']}\"\n",
    "    assert wand_end_framesCount == wand_tip_framesCount, f'frames not matched for {annotation[\"id\"]}'\n",
    "\n",
    "    framesCount = wand_end_framesCount\n",
    "    # assert boxes[0]['value']['framesCount'] == boxes[1]['value']['framesCount'], f'frames not matched for {annotation[\"id\"]}'\n",
    "    # assert len(boxes) >= 2, f\"missing annotations for {annotation['id']}\"\n",
    "\n",
    "    \n",
    "    getCenter(wand_end_keypoint)\n",
    "\n",
    "    wand_end_keypoint = interpolation(wand_end_keypoint, framesCount, False)\n",
    "\n",
    "\n",
    "    getCenter(wand_tip_keypoint)\n",
    "\n",
    "    wand_tip_keypoint = interpolation(wand_tip_keypoint, framesCount, True)\n",
    "\n",
    "\n",
    "    concat_keypoint = np.zeros((framesCount, 5))\n",
    "\n",
    "    concat_keypoint[:, :2] = wand_end_keypoint\n",
    "    concat_keypoint[:, 2:] = wand_tip_keypoint\n",
    "\n",
    "    \n",
    "    keypoints_mapping[vid_name] = torch.tensor(concat_keypoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints_mapping['1f0d52f0-IMG_6458_9.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49926b6d-2023-03-26_05_23_04.mp4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation['file_upload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory that contains original videos\n",
    "\n",
    "import os\n",
    "source_dir = \"G:/.shortcut-targets-by-id/1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT/Data Wizards/dataset/processed_vid\"\n",
    "category_mapping = {\"3-24 V\": 0, \"3-25 bridge\": 1, \"3-25 R\": 2, \"Accio\": 1, \"Avada Kedavra\": 3, \"Invalid\": 4, \"Lumos\": 0, \"Revelio\": 2}\n",
    "\n",
    "vid_class = {} # name in processed_vid : category\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    tmp_root = root[root.rfind('/') + 1: ]\n",
    "    tmp_root = tmp_root[tmp_root.rfind('\\\\') + 1: ]\n",
    "    category = None if tmp_root not in category_mapping.keys() else category_mapping[tmp_root]\n",
    "    for name in files:\n",
    "        if not name.endswith('mp4'):\n",
    "            continue\n",
    "        assert category is not None, f\"No label at{os.path.join(root, name)} {tmp_root}\"\n",
    "\n",
    "        vid_class[name] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_key = 'ground_truth'\n",
    "# vid_class_full = {} # name in video_sync : category \n",
    "\n",
    "# for i in annotations:\n",
    "#     assert 'annotations' in i.keys() and i['annotations'], f\"Empty annotations at {i['id']}\"\n",
    "#     assert 'video' in i['data'].keys()\n",
    "    \n",
    "#     file_path = i['data']['video']\n",
    "\n",
    "#     file_name = file_path[file_path.rfind('/') + 1: ]\n",
    "\n",
    "#     file_name_trim =  file_name[file_name.find('-') + 1:]\n",
    "\n",
    "#     assert file_name_trim in vid_class.keys(), f\"labels not found for {i['id']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") \n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "model_name = \"x3d_s\"\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "frames_per_second = 30\n",
    "model_transform_params  = {\n",
    "    \"x3d_xs\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 4,\n",
    "        \"sampling_rate\": 12,\n",
    "    },\n",
    "    \"x3d_s\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 13,\n",
    "        \"sampling_rate\": 6,\n",
    "    },\n",
    "    \"x3d_m\": {\n",
    "        \"side_size\": 256,\n",
    "        \"crop_size\": 256,\n",
    "        \"num_frames\": 16,\n",
    "        \"sampling_rate\": 5,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get transform parameters based on model\n",
    "transform_params = model_transform_params[model_name]\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            # UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
    "            CenterCropVideo(\n",
    "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_temporal_subsample(\n",
    "    x: torch.Tensor, num_samples: int, temporal_dim: int = -3\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n",
    "    When num_samples is larger than the size of temporal dimension of the video, it\n",
    "    will sample frames based on nearest neighbor interpolation.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): A video tensor with dimension larger than one with torch\n",
    "            tensor type includes int, long, float, complex, etc.\n",
    "        num_samples (int): The number of equispaced samples to be selected\n",
    "        temporal_dim (int): dimension of temporal to perform temporal subsample.\n",
    "\n",
    "    Returns:\n",
    "        An x-like Tensor with subsampled temporal dimension.\n",
    "    \"\"\"\n",
    "    t = x.shape[temporal_dim]\n",
    "    assert num_samples > 0 and t > 0\n",
    "    # Sample by nearest neighbor interpolation if num_samples > t.\n",
    "    indices = torch.linspace(0, t - 1, num_samples)\n",
    "    indices = torch.clamp(indices, 0, t - 1).long()\n",
    "    return torch.index_select(x, temporal_dim, indices), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# vid_path = 'G:\\\\.shortcut-targets-by-id\\\\1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT\\\\Data Wizards\\\\dataset\\\\videoSync\\\\0a1aad14-IMG_1629.mp4'\n",
    "\n",
    "# video = EncodedVideo.from_path(vid_path)\n",
    "\n",
    "# video_data = video.get_clip(start_sec=0, end_sec=3)\n",
    "\n",
    "# uniform_temporal_subsample(video_data['video'], 13)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 289/395 [05:21<01:56,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647db7de-0a1aad14-IMG_1629.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 315/395 [05:51<01:36,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ec71381-04-06-2023-31_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 318/395 [05:54<01:16,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05c834cd-04-06-2023-32_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 333/395 [06:11<01:15,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8728fbe4-04-06-2023-30_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 360/395 [06:43<00:43,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3da0b39-04-06-2023-8_1.mp4\n",
      "475a799b-04-06-2023-4_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 374/395 [06:57<00:25,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68bf2412-04-06-2023-6_1.mp4\n",
      "6f9f9743-04-06-2023-15_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 378/395 [06:59<00:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e61c79ee-04-06-2023-7_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 385/395 [07:07<00:10,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f4da5e75-04-06-2023-3_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [07:16<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "import gc\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "\n",
    "vid_file = 'G:\\\\.shortcut-targets-by-id\\\\1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT\\\\Data Wizards\\\\dataset\\\\videoSync'\n",
    "\n",
    "vids_tensor = []\n",
    "vids_category = []\n",
    "vids_keypoints = []\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(vid_file):\n",
    "    for name in tqdm.tqdm(files):\n",
    "        vid_path = os.path.join(root, name)\n",
    "        trim_name = name[name.find('-') + 1:]\n",
    "        if not vid_path.endswith('.mp4') or trim_name not in vid_class.keys():\n",
    "            print(name)\n",
    "            continue\n",
    "        video = EncodedVideo.from_path(vid_path)\n",
    "        video_data = video.get_clip(start_sec=0, end_sec=3)\n",
    "        del video\n",
    "        gc.collect()\n",
    "        video_cropped, indices = uniform_temporal_subsample(video_data['video'], transform_params[\"num_frames\"])\n",
    "        vids_tensor.append(transform({'video':video_cropped})['video'])\n",
    "        vids_category.append(vid_class[trim_name])\n",
    "        if name in keypoints_mapping:\n",
    "            vids_keypoints.append(keypoints_mapping[name][indices])\n",
    "        else:\n",
    "            vids_keypoints.append(torch.zeros(size=(transform_params[\"num_frames\"], 5)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "size = len(vid_class)\n",
    "\n",
    "vids_concat = list(zip(vids_tensor, vids_keypoints, vids_category))\n",
    "\n",
    "random.shuffle(vids_concat)\n",
    "\n",
    "train_data = vids_concat[:int(size * 0.85)]\n",
    "val_data = vids_concat[int(size * 0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VidClsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]\n",
    "    \n",
    "train_dataset = VidClsDataset(train_data)\n",
    "val_dataset = VidClsDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = x3d_with_regression(pretrained = True, hidden_dim= 6912, model_num_class = 5, head_activation = None).to(device)\n",
    "# model = x3d_s(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "beta = 0.1\n",
    "epoch = 2\n",
    "num_categories = 5\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr= 1e-5)\n",
    "train_size = len(vids_tensor)\n",
    "steps = math.ceil(train_size / batch_size)\n",
    "crossEntropy = CrossEntropyLoss()\n",
    "\n",
    "# input = [torch.zeros(size=(batch_size, 3, slow_num_frames, crop_size, crop_size), device= device), \n",
    "#         torch.zeros(size=(batch_size, 3, num_frames, crop_size, crop_size), device= device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, train_dataloader, val_dataloader):\n",
    "    for epoch_i in range(0, epoch):\n",
    "        model.train()\n",
    "    \n",
    "        loss_list = []\n",
    "        all = 0\n",
    "        correct = 0\n",
    "\n",
    "    \n",
    "        for vid, keypoint, target in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "            # print(input[0].shape)\n",
    "            vid = vid.to(device)\n",
    "            keypoint = keypoint.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "\n",
    "            output = model(vid)\n",
    "        \n",
    "        \n",
    "            cls_loss = crossEntropy(output[0], target)\n",
    "\n",
    "            reg_loss = mse_loss(output[1], keypoint[:,:,:4], keypoint[:,:,4]) * beta\n",
    "\n",
    "            loss = cls_loss\n",
    "            if not reg_loss.isnan().cpu().item():\n",
    "                loss += reg_loss\n",
    "\n",
    "\n",
    "            # print(reg_loss)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output[0], dim= 1) == target).item()\n",
    "            all += vid.shape[0]\n",
    "\n",
    "\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            loss_list.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            del vid\n",
    "            del keypoint\n",
    "            del target\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        print(epoch_i,' train loss:', np.mean(loss_list))\n",
    "        print('train acc:', correct / all)\n",
    "    \n",
    "\n",
    "        correct = 0\n",
    "        all = 0\n",
    "\n",
    "        model.eval()\n",
    "        for vid, keypoint, target in tqdm.tqdm(val_dataloader):\n",
    "\n",
    "            # print(input[0].shape)\n",
    "            vid = vid.to(device)\n",
    "            keypoint = keypoint.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "\n",
    "            output = model(vid)\n",
    "\n",
    "            # print(reg_loss)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output[0], dim= 1) == target).item()\n",
    "            all += vid.shape[0]\n",
    "\n",
    "            del vid\n",
    "            del keypoint\n",
    "            del target\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        print('val acc:', correct / all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train loss: 1.5955128261021205\n",
      "train acc: 0.28776978417266186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.330188679245283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  train loss: 1.4486249583108084\n",
      "train acc: 0.49640287769784175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.5754716981132075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  train loss: 1.2992426242147173\n",
      "train acc: 0.7050359712230215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6792452830188679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  train loss: 1.144862448317664\n",
      "train acc: 0.7266187050359713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6698113207547169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  train loss: 1.0108079663344791\n",
      "train acc: 0.7553956834532374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6698113207547169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  train loss: 0.8881192105157035\n",
      "train acc: 0.7985611510791367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6792452830188679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  train loss: 0.7715329787560872\n",
      "train acc: 0.8309352517985612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.7547169811320755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  train loss: 0.6423448319946017\n",
      "train acc: 0.8129496402877698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.7924528301886793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  train loss: 0.577091532094138\n",
      "train acc: 0.841726618705036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.7830188679245284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  train loss: 0.5078424049275262\n",
      "train acc: 0.8812949640287769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8584905660377359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  train loss: 0.4668771888528551\n",
      "train acc: 0.8848920863309353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:09<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8584905660377359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  train loss: 0.44497772572296007\n",
      "train acc: 0.8669064748201439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8584905660377359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12  train loss: 0.38802718264716013\n",
      "train acc: 0.9028776978417267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8679245283018868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13  train loss: 0.34742201971156256\n",
      "train acc: 0.9100719424460432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8584905660377359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  train loss: 0.31858099890606745\n",
      "train acc: 0.9316546762589928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8584905660377359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  train loss: 0.282576433090227\n",
      "train acc: 0.9172661870503597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  train loss: 0.25782322686697756\n",
      "train acc: 0.9460431654676259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17  train loss: 0.2595900952283825\n",
      "train acc: 0.9316546762589928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  train loss: 0.2208694228636367\n",
      "train acc: 0.935251798561151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  train loss: 0.1833762344771198\n",
      "train acc: 0.9568345323741008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, 20, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train loss: 0.18964495190552302\n",
      "train acc: 0.9712230215827338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  train loss: 0.15392648026879344\n",
      "train acc: 0.9784172661870504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  train loss: 0.1527777915288295\n",
      "train acc: 0.9748201438848921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  train loss: 0.10323269681206772\n",
      "train acc: 0.9928057553956835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  train loss: 0.097722680707063\n",
      "train acc: 0.9928057553956835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9433962264150944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  train loss: 0.1012622277252376\n",
      "train acc: 0.9784172661870504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  train loss: 0.0953673047412719\n",
      "train acc: 0.9892086330935251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9245283018867925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  train loss: 0.09923220352668848\n",
      "train acc: 0.9892086330935251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:21<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  train loss: 0.11529366948774883\n",
      "train acc: 0.9784172661870504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  train loss: 0.07950218069101019\n",
      "train acc: 0.9892086330935251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  train loss: 0.06356943374765771\n",
      "train acc: 0.9964028776978417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  train loss: 0.05154777321565364\n",
      "train acc: 0.9964028776978417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12  train loss: 0.04982759889348277\n",
      "train acc: 0.9964028776978417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13  train loss: 0.05568362303809928\n",
      "train acc: 0.9856115107913669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9245283018867925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  train loss: 0.031931557369950625\n",
      "train acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:20<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  train loss: 0.044167224261244496\n",
      "train acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  train loss: 0.03647206047815936\n",
      "train acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17  train loss: 0.049310640039454616\n",
      "train acc: 0.9856115107913669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  train loss: 0.03312025060211973\n",
      "train acc: 0.9964028776978417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:19<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  train loss: 0.04727803177000689\n",
      "train acc: 0.9964028776978417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:10<00:00, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9150943396226415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, 20, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.zeros((1,1), device=device)\n",
    "type(tmp[tmp != 0].mean().isnan().cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6006215810775757,\n",
       " 1.6522184610366821,\n",
       " 1.5508222579956055,\n",
       " 1.5435348749160767,\n",
       " 1.5520023107528687,\n",
       " 1.5453161001205444,\n",
       " 1.5513572692871094,\n",
       " 1.5396989583969116,\n",
       " 1.5261726379394531,\n",
       " 1.53245210647583,\n",
       " 1.4759129285812378,\n",
       " 1.4179316759109497,\n",
       " 1.5488102436065674,\n",
       " 1.526351809501648,\n",
       " 1.5203030109405518,\n",
       " 1.4540609121322632,\n",
       " 1.5517607927322388,\n",
       " 1.5476685762405396,\n",
       " 1.5017987489700317,\n",
       " 1.5352576971054077,\n",
       " 1.560221791267395,\n",
       " 1.5328083038330078,\n",
       " 1.5226811170578003,\n",
       " 1.502122402191162,\n",
       " 1.433959722518921,\n",
       " 1.4614757299423218,\n",
       " 1.5298354625701904,\n",
       " 1.3777358531951904,\n",
       " 1.4819597005844116,\n",
       " 1.6072654724121094,\n",
       " 1.4554775953292847,\n",
       " 1.4510821104049683,\n",
       " 1.4250015020370483,\n",
       " 1.4322353601455688,\n",
       " 1.4579918384552002]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keypoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keypoint[:,:,:\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mview((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m))\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keypoint' is not defined"
     ]
    }
   ],
   "source": [
    "keypoint[:,:,:4].view((-1, 4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"x3d_regression_4_11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1644, -0.0496, -0.0880,  0.0246, -0.2376]], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[ 0.7603, -1.8393, -2.2067],\n",
       "         [ 0.8296, -1.9309, -2.0818],\n",
       "         [ 0.8647, -0.9451, -1.9459],\n",
       "         [ 1.3491, -0.5821, -1.8753],\n",
       "         [ 1.3601, -0.4220, -2.1563],\n",
       "         [ 1.4539, -0.9777, -2.2140],\n",
       "         [ 1.6425, -0.9868, -2.0187],\n",
       "         [ 1.5443, -0.8109, -2.4173],\n",
       "         [ 1.7484, -0.7446, -1.9516],\n",
       "         [ 1.8273, -0.3302, -2.0197],\n",
       "         [ 1.3747, -1.2899, -1.1885],\n",
       "         [ 0.3637, -2.4861, -0.8878],\n",
       "         [-0.0671, -2.1500, -0.9555]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.zeros(size=vids_tensor[0].shape, device=device).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "566_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
