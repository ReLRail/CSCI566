{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.models.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.verify(\"ResNet18_Weights.IMAGENET1K_V1\")\n",
    "state_dict = weights.get_state_dict(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 96\n",
    "\n",
    "backbone = torchvision.models.resnet18(num_classes = embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    for i, j in backbone.named_parameters():\n",
    "        if i in state_dict.keys() and j.shape == state_dict[i].shape:\n",
    "            j[:] = state_dict[i]\n",
    "            # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127,  0.0111,\n",
       "         0.0095, -0.1099])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.state_dict()['conv1.weight'].view((-1,))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127,  0.0111,\n",
       "         0.0095, -0.1099], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['conv1.weight'].view((-1,))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone(torch.zeros(size=(1, 3, 640, 640))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.models.retinanet_resnet50_fpn\n",
    "# torchvision.models.resnet18.__name__\n",
    "# torchvision.models.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Token e4342ac4fcf98c2e1910b122cb4103c059f8bbfc',\n",
    "}\n",
    "\n",
    "response = requests.get('https://bilishorturl.ml/api/projects/3/export?exportType=JSON', headers=headers)\n",
    "\n",
    "import json\n",
    "annotations = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "keypoints_mapping = {}\n",
    "\n",
    "def getCenter(keypoints):\n",
    "    for point in keypoints:\n",
    "        point['center_x'] = point['x'] + point['width'] / 2 \n",
    "        point['center_y'] = point['y'] + point['height'] / 2\n",
    "\n",
    "# return_interpolation: When true append whether interpolated at the end\n",
    "# 1 means exist, 0 means missing\n",
    "def interpolation(keypoints, frames, return_interpolation):\n",
    "    prev = keypoints[0]['frame'] - 1\n",
    "    prev_x = 0\n",
    "    prev_y = 0\n",
    "    res = np.zeros((frames, 3 if return_interpolation else 2))\n",
    "    for i in keypoints:\n",
    "        diff = i['frame'] - prev\n",
    "        cur_x = i['center_x']\n",
    "        cur_y = i['center_y']\n",
    "        cur = i['frame']\n",
    "        for j in range(prev + 1, i['frame']):\n",
    "            # tmp = {'frame': j}\n",
    "            tmp_x = (prev_x * (cur - j) + cur_x * (j - prev)) / diff\n",
    "            tmp_y = (prev_y * (cur - j) + cur_y * (j - prev)) / diff\n",
    "\n",
    "            res[j - 1, :2] = (tmp_x / 100, tmp_y / 100)\n",
    "            if return_interpolation:\n",
    "                res[j - 1, -1] = 1\n",
    "            # tmp['interpolated'] = True\n",
    "            # res.append(tmp)\n",
    "        res[cur - 1, :2] = (cur_x / 100, cur_y / 100)\n",
    "        if return_interpolation:\n",
    "            res[cur - 1, -1] = 1\n",
    "        prev_x = cur_x\n",
    "        prev_y = cur_y\n",
    "        prev = i['frame']\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "labels_name = ['wand tip', 'wand end']\n",
    "for annotation in annotations:\n",
    "    vid_name = annotation['file_upload']\n",
    "\n",
    "    boxes = annotation['annotations'][0]['result']\n",
    "    \n",
    "    wand_end_keypoint = None\n",
    "    wand_tip_keypoint = None\n",
    "    wand_end_framesCount = None\n",
    "    wand_tip_framesCount = None\n",
    "\n",
    "    for i in boxes:\n",
    "        if 'labels' not in i['value'].keys():\n",
    "            continue\n",
    "        if i['value']['labels'][0] == labels_name[0]:\n",
    "            wand_tip_keypoint = i['value']['sequence']\n",
    "            wand_tip_framesCount = i['value']['framesCount']\n",
    "        elif i['value']['labels'][0] == labels_name[1]:\n",
    "            wand_end_keypoint = i['value']['sequence']\n",
    "            wand_end_framesCount = i['value']['framesCount']\n",
    "    \n",
    "    assert wand_tip_keypoint and wand_end_keypoint, f\"missing annotations for {annotation['id']}\"\n",
    "    assert wand_end_framesCount == wand_tip_framesCount, f'frames not matched for {annotation[\"id\"]}'\n",
    "\n",
    "    framesCount = wand_end_framesCount\n",
    "    # assert boxes[0]['value']['framesCount'] == boxes[1]['value']['framesCount'], f'frames not matched for {annotation[\"id\"]}'\n",
    "    # assert len(boxes) >= 2, f\"missing annotations for {annotation['id']}\"\n",
    "\n",
    "    \n",
    "    getCenter(wand_end_keypoint)\n",
    "\n",
    "    wand_end_keypoint = interpolation(wand_end_keypoint, framesCount, False)\n",
    "\n",
    "\n",
    "    getCenter(wand_tip_keypoint)\n",
    "\n",
    "    wand_tip_keypoint = interpolation(wand_tip_keypoint, framesCount, True)\n",
    "\n",
    "\n",
    "    concat_keypoint = np.zeros((framesCount, 5))\n",
    "\n",
    "    concat_keypoint[:, :2] = wand_end_keypoint\n",
    "    concat_keypoint[:, 2:] = wand_tip_keypoint\n",
    "\n",
    "    \n",
    "    keypoints_mapping[vid_name] = torch.tensor(concat_keypoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory that contains original videos\n",
    "\n",
    "import os\n",
    "source_dir = \"G:/.shortcut-targets-by-id/1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT/Data Wizards/dataset/processed_vid\"\n",
    "category_mapping = {\"3-24 V\": 0, \"3-25 bridge\": 1, \"3-25 R\": 2, \"Accio\": 1, \"Avada Kedavra\": 3, \"Invalid\": 4, \"Lumos\": 0, \"Revelio\": 2}\n",
    "\n",
    "vid_class = {} # name in processed_vid : category\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    tmp_root = root[root.rfind('/') + 1: ]\n",
    "    tmp_root = tmp_root[tmp_root.rfind('\\\\') + 1: ]\n",
    "    category = None if tmp_root not in category_mapping.keys() else category_mapping[tmp_root]\n",
    "    for name in files:\n",
    "        if not name.endswith('mp4'):\n",
    "            continue\n",
    "        assert category is not None, f\"No label at{os.path.join(root, name)} {tmp_root}\"\n",
    "\n",
    "        vid_class[name] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29197\\miniconda3\\envs\\566_new\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") \n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "model_name = \"x3d_m\"\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "frames_per_second = 30\n",
    "model_transform_params  = {\n",
    "    \"x3d_xs\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 4,\n",
    "        \"sampling_rate\": 12,\n",
    "    },\n",
    "    \"x3d_s\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 13,\n",
    "        \"sampling_rate\": 6,\n",
    "    },\n",
    "    \"x3d_m\": {\n",
    "        \"side_size\": 224,\n",
    "        \"crop_size\": 224,\n",
    "        \"num_frames\": 16,\n",
    "        \"sampling_rate\": 5,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get transform parameters based on model\n",
    "transform_params = model_transform_params[model_name]\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            # UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
    "            CenterCropVideo(\n",
    "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_temporal_subsample(\n",
    "    x: torch.Tensor, num_samples: int, temporal_dim: int = -3\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n",
    "    When num_samples is larger than the size of temporal dimension of the video, it\n",
    "    will sample frames based on nearest neighbor interpolation.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): A video tensor with dimension larger than one with torch\n",
    "            tensor type includes int, long, float, complex, etc.\n",
    "        num_samples (int): The number of equispaced samples to be selected\n",
    "        temporal_dim (int): dimension of temporal to perform temporal subsample.\n",
    "\n",
    "    Returns:\n",
    "        An x-like Tensor with subsampled temporal dimension.\n",
    "    \"\"\"\n",
    "    t = x.shape[temporal_dim]\n",
    "    assert num_samples > 0 and t > 0\n",
    "    # Sample by nearest neighbor interpolation if num_samples > t.\n",
    "    indices = torch.linspace(0, t - 1, num_samples)\n",
    "    indices = torch.clamp(indices, 0, t - 1).long()\n",
    "    return torch.index_select(x, temporal_dim, indices), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 289/395 [05:46<02:13,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647db7de-0a1aad14-IMG_1629.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 315/395 [06:20<01:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ec71381-04-06-2023-31_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 318/395 [06:22<01:24,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05c834cd-04-06-2023-32_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 333/395 [06:41<01:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8728fbe4-04-06-2023-30_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 360/395 [07:16<00:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3da0b39-04-06-2023-8_1.mp4\n",
      "475a799b-04-06-2023-4_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 374/395 [07:31<00:27,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68bf2412-04-06-2023-6_1.mp4\n",
      "6f9f9743-04-06-2023-15_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 378/395 [07:34<00:16,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e61c79ee-04-06-2023-7_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 385/395 [07:42<00:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f4da5e75-04-06-2023-3_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [07:52<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "import gc\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "\n",
    "vid_file = 'G:\\\\.shortcut-targets-by-id\\\\1eyTB0qCfXgrxNsrmWNeLNbd5sTKzP5HT\\\\Data Wizards\\\\dataset\\\\videoSync'\n",
    "\n",
    "vids_tensor = []\n",
    "vids_category = []\n",
    "vids_keypoints = []\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(vid_file):\n",
    "    for name in tqdm.tqdm(files):\n",
    "        vid_path = os.path.join(root, name)\n",
    "        trim_name = name[name.find('-') + 1:]\n",
    "        if not vid_path.endswith('.mp4') or trim_name not in vid_class.keys():\n",
    "            print(name)\n",
    "            continue\n",
    "        video = EncodedVideo.from_path(vid_path)\n",
    "        video_data = video.get_clip(start_sec=0, end_sec=3)\n",
    "        del video\n",
    "        gc.collect()\n",
    "        video_cropped, indices = uniform_temporal_subsample(video_data['video'], transform_params[\"num_frames\"])\n",
    "        vids_tensor.append(transform({'video':video_cropped})['video'])\n",
    "        vids_category.append(vid_class[trim_name])\n",
    "        if name in keypoints_mapping:\n",
    "            vids_keypoints.append(keypoints_mapping[name][indices])\n",
    "        else:\n",
    "            vids_keypoints.append(torch.zeros(size=(transform_params[\"num_frames\"], 5)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "size = len(vid_class)\n",
    "\n",
    "vids_concat = list(zip(vids_tensor, vids_keypoints, vids_category))\n",
    "\n",
    "random.shuffle(vids_concat)\n",
    "\n",
    "train_data = vids_concat[:int(size * 0.85)]\n",
    "val_data = vids_concat[int(size * 0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VidClsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]\n",
    "    \n",
    "train_dataset = VidClsDataset(train_data)\n",
    "val_dataset = VidClsDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "y = None\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self, backbone,embed_size = embed_size, in_between = 64, out_features = 4, num_layers = 2):\n",
    "        super(lstm, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.backbone = backbone\n",
    "        # self.embedding = nn.Embedding(x_grid_size * y_grid_size + 1, embed_size)\n",
    "        self.rnn = nn.LSTM(input_size = embed_size, hidden_size = 150, batch_first = True, bidirectional = True, dropout = 0.1, num_layers = num_layers)\n",
    "        # self.rnn2 = nn.LSTM(input_size = embed_size, hidden_size = 100, batch_first = True, bidirectional = True, dropout = 0.1, num_layers = num_layers)\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(300, 200)\n",
    "        self.dropout2 = nn.Dropout(p = 0.1)\n",
    "        self.fc2 = nn.Linear(200, 5)\n",
    "\n",
    "\n",
    "        self.keypoint_head_1 = nn.Linear(in_features=embed_size, out_features=in_between)\n",
    "        self.keypoint_head_2 = nn.Linear(in_features=in_between, out_features = out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.rnn(x)[1][0].shape)\n",
    "        # return self.fc(torch.squeeze(self.rnn(x)[1][0], dim=0))\n",
    "        # print(self.rnn(x)[0].shape)\n",
    "        global y\n",
    "        y = x\n",
    "        a, b, c, d, e = x.shape\n",
    "        features = backbone(x.view((a * b, c, d, e))).view((a, b, self.embed_size))\n",
    "        return self.fc2(self.dropout2(self.relu( \\\n",
    "            self.fc1(self.dropout(self.rnn(features)[0][:, -1, :]))))) , \\\n",
    "         self.keypoint_head_2(nn.ReLU()(self.keypoint_head_1(features.view((a * b, self.embed_size)))))\n",
    "\n",
    "\n",
    "\n",
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"rnn\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "beta = 0.1\n",
    "epoch = 2\n",
    "num_categories = 5\n",
    "\n",
    "\n",
    "\n",
    "train_size = len(vids_tensor)\n",
    "steps = math.ceil(train_size / batch_size)\n",
    "crossEntropy = CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda'\n",
    "model = lstm(backbone=backbone).to(device)\n",
    "optimizer = Adam(get_grouped_params(model, weight_decay = 0.01), lr= 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target, mask, reduction=\"mean\"):\n",
    "    out = (input - target.view((-1, 4)))**2\n",
    "    # print(out.view((-1, 4)).shape)\n",
    "    # print(mask.view((-1,)).shape)\n",
    "    out = out.view((-1, 4)) * mask.view((-1, 1))\n",
    "    if reduction == \"mean\":\n",
    "        return out[out != 0].mean()\n",
    "    elif reduction == \"None\":\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "\n",
    "def train(model, epoch, optimizer, train_dataloader, val_dataloader):\n",
    "    global output\n",
    "    for epoch_i in range(0, epoch):\n",
    "        model.train()\n",
    "    \n",
    "        loss_list = []\n",
    "        all = 0\n",
    "        correct = 0\n",
    "\n",
    "    \n",
    "        for vid, keypoint, target in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "            # print(input[0].shape)\n",
    "            vid = vid.permute((0,2,1,3,4)).contiguous().to(device)\n",
    "            keypoint = keypoint.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "\n",
    "            output = model(vid)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "            cls_loss = crossEntropy(output[0], target)\n",
    "\n",
    "            reg_loss = mse_loss(output[1], keypoint[:,:,:4], keypoint[:,:,4]) * beta\n",
    "\n",
    "            loss = cls_loss\n",
    "            if not reg_loss.isnan().cpu().item():\n",
    "                loss += reg_loss\n",
    "\n",
    "\n",
    "            # print(reg_loss)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output[0], dim= 1) == target).item()\n",
    "            all += vid.shape[0]\n",
    "\n",
    "\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            loss_list.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            del vid\n",
    "            del keypoint\n",
    "            del target\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        print(epoch_i,' train loss:', np.mean(loss_list))\n",
    "        print('train acc:', correct / all)\n",
    "    \n",
    "\n",
    "        correct = 0\n",
    "        all = 0\n",
    "\n",
    "        model.eval()\n",
    "        for vid, keypoint, target in tqdm.tqdm(val_dataloader):\n",
    "\n",
    "            # print(input[0].shape)\n",
    "            vid = vid.permute((0,2,1,3,4)).contiguous().to(device)\n",
    "            keypoint = keypoint.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "\n",
    "            output = model(vid)\n",
    "\n",
    "            # print(reg_loss)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output[0], dim= 1) == target).item()\n",
    "            all += vid.shape[0]\n",
    "\n",
    "            del vid\n",
    "            del keypoint\n",
    "            del target\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        print('val acc:', correct / all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train loss: 1.6094906040600367\n",
      "train acc: 0.20863309352517986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:14<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.27358490566037735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  train loss: 1.5955458964620317\n",
      "train acc: 0.31654676258992803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.44339622641509435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  train loss: 1.5769921728542873\n",
      "train acc: 0.42805755395683454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.5188679245283019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  train loss: 1.5458644134657724\n",
      "train acc: 0.5251798561151079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.5754716981132075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  train loss: 1.4890140226909092\n",
      "train acc: 0.6151079136690647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.660377358490566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  train loss: 1.4075806907245092\n",
      "train acc: 0.6798561151079137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6886792452830188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  train loss: 1.2842665902205876\n",
      "train acc: 0.737410071942446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.6886792452830188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  train loss: 1.1817974303449903\n",
      "train acc: 0.7338129496402878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.7075471698113207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  train loss: 1.0606905102729798\n",
      "train acc: 0.7949640287769785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.839622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  train loss: 0.9228447462831225\n",
      "train acc: 0.8597122302158273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  train loss: 0.8171799578836986\n",
      "train acc: 0.8741007194244604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9433962264150944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  train loss: 0.7264001535517829\n",
      "train acc: 0.8848920863309353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8679245283018868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12  train loss: 0.589730503303664\n",
      "train acc: 0.9172661870503597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13  train loss: 0.49310784637928007\n",
      "train acc: 0.9496402877697842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9528301886792453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  train loss: 0.4203102420483317\n",
      "train acc: 0.9640287769784173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9056603773584906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  train loss: 0.3752299483333315\n",
      "train acc: 0.9568345323741008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  train loss: 0.32764080315828326\n",
      "train acc: 0.9712230215827338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:11<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17  train loss: 0.2665068550833634\n",
      "train acc: 0.9820143884892086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:13<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.8867924528301887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  train loss: 0.24312072470784188\n",
      "train acc: 0.9748201438848921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9339622641509434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:18<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  train loss: 0.1958825582904475\n",
      "train acc: 0.9820143884892086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:12<00:00,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.9245283018867925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, 20, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0065, -0.0238, -0.0570, -0.1022, -0.0679],\n",
       "        [ 0.0018, -0.0277, -0.0492, -0.0985, -0.0915],\n",
       "        [-0.0071, -0.0186, -0.0418, -0.0832, -0.0755],\n",
       "        [-0.0008, -0.0127, -0.0558, -0.1048, -0.0637]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100352.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9633792/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e = y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3, 224, 224)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * b, c, d, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view((a * b, c, d, e)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0381, -0.0470,  0.0389,  ..., -0.1045, -0.0727,  0.0261],\n",
       "          [ 0.0414, -0.0652,  0.0639,  ..., -0.0921, -0.0698,  0.0198],\n",
       "          [ 0.0383, -0.0728,  0.0718,  ..., -0.0754, -0.0642,  0.0262],\n",
       "          ...,\n",
       "          [ 0.0623, -0.0517,  0.1028,  ..., -0.0656, -0.0451,  0.0191],\n",
       "          [ 0.0457, -0.0425,  0.1122,  ..., -0.0500, -0.0242,  0.0156],\n",
       "          [ 0.0275, -0.0305,  0.1014,  ..., -0.0336, -0.0203,  0.0078]],\n",
       " \n",
       "         [[ 0.0089, -0.0407,  0.0113,  ..., -0.1021, -0.1198, -0.0263],\n",
       "          [ 0.0377, -0.0600,  0.0329,  ..., -0.1021, -0.1205, -0.0343],\n",
       "          [ 0.0695, -0.0767,  0.0213,  ..., -0.1146, -0.1365, -0.0347],\n",
       "          ...,\n",
       "          [ 0.0434, -0.0558,  0.0411,  ..., -0.0812, -0.0770, -0.0266],\n",
       "          [ 0.0370, -0.0373,  0.0368,  ..., -0.0478, -0.0658, -0.0040],\n",
       "          [ 0.0338, -0.0288,  0.0367,  ..., -0.0515, -0.0481,  0.0009]],\n",
       " \n",
       "         [[ 0.0119, -0.0397,  0.0095,  ..., -0.0550, -0.1150,  0.0045],\n",
       "          [ 0.0088, -0.0602,  0.0220,  ..., -0.0599, -0.1243,  0.0152],\n",
       "          [ 0.0254, -0.0723,  0.0403,  ..., -0.0588, -0.1179,  0.0206],\n",
       "          ...,\n",
       "          [ 0.0564, -0.0809,  0.0584,  ..., -0.0318, -0.1567, -0.0111],\n",
       "          [ 0.0258, -0.0633,  0.0485,  ..., -0.0359, -0.1308, -0.0168],\n",
       "          [ 0.0253, -0.0561,  0.0558,  ..., -0.0200, -0.0796, -0.0029]],\n",
       " \n",
       "         [[ 0.0219, -0.0131, -0.0066,  ..., -0.0375, -0.1267, -0.0393],\n",
       "          [ 0.0406, -0.0369, -0.0088,  ..., -0.0315, -0.1159, -0.0532],\n",
       "          [ 0.0572, -0.0263, -0.0048,  ..., -0.0482, -0.1282, -0.0586],\n",
       "          ...,\n",
       "          [ 0.0870, -0.0416,  0.0401,  ..., -0.0544, -0.0904,  0.0140],\n",
       "          [ 0.0568, -0.0352,  0.0529,  ..., -0.0645, -0.0867,  0.0102],\n",
       "          [ 0.0539, -0.0231,  0.0565,  ..., -0.0379, -0.0566, -0.0052]]],\n",
       "        device='cuda:0', grad_fn=<CudnnRnnBackward0>),\n",
       " (tensor([[[ 0.2479,  0.1977,  0.0767,  ..., -0.0891,  0.1242,  0.0123],\n",
       "           [ 0.2826,  0.2011, -0.0066,  ..., -0.0216,  0.2286, -0.0377],\n",
       "           [ 0.3006,  0.0912,  0.0258,  ..., -0.1547,  0.1114,  0.0593],\n",
       "           [ 0.2529,  0.1448, -0.0017,  ..., -0.0455,  0.1464, -0.0285]],\n",
       "  \n",
       "          [[-0.1859, -0.0124, -0.2418,  ..., -0.2004, -0.0197, -0.0047],\n",
       "           [-0.1292, -0.0172,  0.0581,  ..., -0.2011,  0.0343, -0.0547],\n",
       "           [-0.3298, -0.0424,  0.0159,  ..., -0.1233, -0.0374,  0.1618],\n",
       "           [-0.0081, -0.0173,  0.0087,  ..., -0.2073, -0.0214,  0.0926]],\n",
       "  \n",
       "          [[ 0.0275, -0.0305,  0.1014,  ...,  0.0762, -0.0159,  0.0438],\n",
       "           [ 0.0338, -0.0288,  0.0367,  ...,  0.0750, -0.0233,  0.0240],\n",
       "           [ 0.0253, -0.0561,  0.0558,  ...,  0.0289, -0.0438,  0.0572],\n",
       "           [ 0.0539, -0.0231,  0.0565,  ...,  0.0811, -0.0585,  0.0280]],\n",
       "  \n",
       "          [[-0.1099,  0.0965, -0.0026,  ..., -0.1045, -0.0727,  0.0261],\n",
       "           [-0.1057,  0.0672, -0.0271,  ..., -0.1021, -0.1198, -0.0263],\n",
       "           [-0.1445,  0.1199, -0.0763,  ..., -0.0550, -0.1150,  0.0045],\n",
       "           [-0.1074,  0.0655, -0.0095,  ..., -0.0375, -0.1267, -0.0393]]],\n",
       "         device='cuda:0', grad_fn=<CudnnRnnBackward0>),\n",
       "  tensor([[[ 0.5450,  0.3199,  0.1350,  ..., -0.1580,  0.2862,  0.0269],\n",
       "           [ 0.6723,  0.3580, -0.0144,  ..., -0.0408,  0.5636, -0.0842],\n",
       "           [ 0.6822,  0.1493,  0.0526,  ..., -0.2918,  0.2844,  0.1310],\n",
       "           [ 0.5836,  0.2594, -0.0037,  ..., -0.0832,  0.3751, -0.0652]],\n",
       "  \n",
       "          [[-0.3980, -0.0284, -0.4750,  ..., -0.4428, -0.0483, -0.0118],\n",
       "           [-0.2866, -0.0405,  0.0937,  ..., -0.5122,  0.0874, -0.1312],\n",
       "           [-0.7457, -0.1151,  0.0352,  ..., -0.2392, -0.0913,  0.3260],\n",
       "           [-0.0208, -0.0401,  0.0147,  ..., -0.4739, -0.0582,  0.2440]],\n",
       "  \n",
       "          [[ 0.0518, -0.0621,  0.2161,  ...,  0.1529, -0.0320,  0.0932],\n",
       "           [ 0.0688, -0.0592,  0.0797,  ...,  0.1535, -0.0459,  0.0512],\n",
       "           [ 0.0490, -0.1168,  0.1169,  ...,  0.0541, -0.0898,  0.1174],\n",
       "           [ 0.1116, -0.0475,  0.1184,  ...,  0.1552, -0.1090,  0.0572]],\n",
       "  \n",
       "          [[-0.2083,  0.2009, -0.0051,  ..., -0.2031, -0.1581,  0.0525],\n",
       "           [-0.1975,  0.1380, -0.0520,  ..., -0.1955, -0.2525, -0.0545],\n",
       "           [-0.2712,  0.2612, -0.1506,  ..., -0.1081, -0.2364,  0.0091],\n",
       "           [-0.1970,  0.1400, -0.0198,  ..., -0.0754, -0.2664, -0.0766]]],\n",
       "         device='cuda:0', grad_fn=<CudnnRnnBackward0>)))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnn(model.backbone(y.view((a * b, c, d, e))).view((4,16,96)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "566_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
